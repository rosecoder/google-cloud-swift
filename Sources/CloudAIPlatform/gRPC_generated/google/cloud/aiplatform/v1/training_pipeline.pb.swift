// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1/training_pipeline.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The TrainingPipeline orchestrates tasks associated with training a Model. It
/// always executes the training task, and optionally may also
/// export data from Vertex AI's Dataset which becomes the training input,
/// [upload][google.cloud.aiplatform.v1.ModelService.UploadModel] the Model to
/// Vertex AI, and evaluate the Model.
public struct Google_Cloud_Aiplatform_V1_TrainingPipeline: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Resource name of the TrainingPipeline.
  public var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// Required. The user-defined name of this TrainingPipeline.
  public var displayName: String {
    get {return _storage._displayName}
    set {_uniqueStorage()._displayName = newValue}
  }

  /// Specifies Vertex AI owned input data that may be used for training the
  /// Model. The TrainingPipeline's
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
  /// should make clear whether this config is used and if there are any special
  /// requirements on how it should be filled. If nothing about this config is
  /// mentioned in the
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition],
  /// then it should be assumed that the TrainingPipeline does not depend on this
  /// configuration.
  public var inputDataConfig: Google_Cloud_Aiplatform_V1_InputDataConfig {
    get {return _storage._inputDataConfig ?? Google_Cloud_Aiplatform_V1_InputDataConfig()}
    set {_uniqueStorage()._inputDataConfig = newValue}
  }
  /// Returns true if `inputDataConfig` has been explicitly set.
  public var hasInputDataConfig: Bool {return _storage._inputDataConfig != nil}
  /// Clears the value of `inputDataConfig`. Subsequent reads from it will return its default value.
  public mutating func clearInputDataConfig() {_uniqueStorage()._inputDataConfig = nil}

  /// Required. A Google Cloud Storage path to the YAML file that defines the
  /// training task which is responsible for producing the model artifact, and
  /// may also include additional auxiliary work. The definition files that can
  /// be used here are found in
  /// gs://google-cloud-aiplatform/schema/trainingjob/definition/.
  /// Note: The URI given on output will be immutable and probably different,
  /// including the URI scheme, than the one given on input. The output URI will
  /// point to a location where the user only has a read access.
  public var trainingTaskDefinition: String {
    get {return _storage._trainingTaskDefinition}
    set {_uniqueStorage()._trainingTaskDefinition = newValue}
  }

  /// Required. The training task's parameter(s), as specified in the
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s
  /// `inputs`.
  public var trainingTaskInputs: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._trainingTaskInputs ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._trainingTaskInputs = newValue}
  }
  /// Returns true if `trainingTaskInputs` has been explicitly set.
  public var hasTrainingTaskInputs: Bool {return _storage._trainingTaskInputs != nil}
  /// Clears the value of `trainingTaskInputs`. Subsequent reads from it will return its default value.
  public mutating func clearTrainingTaskInputs() {_uniqueStorage()._trainingTaskInputs = nil}

  /// Output only. The metadata information as specified in the
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s
  /// `metadata`. This metadata is an auxiliary runtime and final information
  /// about the training task. While the pipeline is running this information is
  /// populated only at a best effort basis. Only present if the
  /// pipeline's
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
  /// contains `metadata` object.
  public var trainingTaskMetadata: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._trainingTaskMetadata ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._trainingTaskMetadata = newValue}
  }
  /// Returns true if `trainingTaskMetadata` has been explicitly set.
  public var hasTrainingTaskMetadata: Bool {return _storage._trainingTaskMetadata != nil}
  /// Clears the value of `trainingTaskMetadata`. Subsequent reads from it will return its default value.
  public mutating func clearTrainingTaskMetadata() {_uniqueStorage()._trainingTaskMetadata = nil}

  /// Describes the Model that may be uploaded (via
  /// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel])
  /// by this TrainingPipeline. The TrainingPipeline's
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
  /// should make clear whether this Model description should be populated, and
  /// if there are any special requirements regarding how it should be filled. If
  /// nothing is mentioned in the
  /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition],
  /// then it should be assumed that this field should not be filled and the
  /// training task either uploads the Model without a need of this information,
  /// or that training task does not support uploading a Model as part of the
  /// pipeline. When the Pipeline's state becomes `PIPELINE_STATE_SUCCEEDED` and
  /// the trained Model had been uploaded into Vertex AI, then the
  /// model_to_upload's resource [name][google.cloud.aiplatform.v1.Model.name] is
  /// populated. The Model is always uploaded into the Project and Location in
  /// which this pipeline is.
  public var modelToUpload: Google_Cloud_Aiplatform_V1_Model {
    get {return _storage._modelToUpload ?? Google_Cloud_Aiplatform_V1_Model()}
    set {_uniqueStorage()._modelToUpload = newValue}
  }
  /// Returns true if `modelToUpload` has been explicitly set.
  public var hasModelToUpload: Bool {return _storage._modelToUpload != nil}
  /// Clears the value of `modelToUpload`. Subsequent reads from it will return its default value.
  public mutating func clearModelToUpload() {_uniqueStorage()._modelToUpload = nil}

  /// Optional. The ID to use for the uploaded Model, which will become the final
  /// component of the model resource name.
  ///
  /// This value may be up to 63 characters, and valid characters are
  /// `[a-z0-9_-]`. The first character cannot be a number or hyphen.
  public var modelID: String {
    get {return _storage._modelID}
    set {_uniqueStorage()._modelID = newValue}
  }

  /// Optional. When specify this field, the `model_to_upload` will not be
  /// uploaded as a new model, instead, it will become a new version of this
  /// `parent_model`.
  public var parentModel: String {
    get {return _storage._parentModel}
    set {_uniqueStorage()._parentModel = newValue}
  }

  /// Output only. The detailed state of the pipeline.
  public var state: Google_Cloud_Aiplatform_V1_PipelineState {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. Only populated when the pipeline's state is
  /// `PIPELINE_STATE_FAILED` or `PIPELINE_STATE_CANCELLED`.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// Output only. Time when the TrainingPipeline was created.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Time when the TrainingPipeline for the first time entered the
  /// `PIPELINE_STATE_RUNNING` state.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Time when the TrainingPipeline entered any of the following
  /// states: `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`,
  /// `PIPELINE_STATE_CANCELLED`.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. Time when the TrainingPipeline was most recently updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return _storage._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {_uniqueStorage()._updateTime = nil}

  /// The labels with user-defined metadata to organize TrainingPipelines.
  ///
  /// Label keys and values can be no longer than 64 characters
  /// (Unicode codepoints), can only contain lowercase letters, numeric
  /// characters, underscores and dashes. International characters are allowed.
  ///
  /// See https://goo.gl/xmQnxf for more information and examples of labels.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  /// Customer-managed encryption key spec for a TrainingPipeline. If set, this
  /// TrainingPipeline will be secured by this key.
  ///
  /// Note: Model trained by this TrainingPipeline is also secured by this key if
  /// [model_to_upload][google.cloud.aiplatform.v1.TrainingPipeline.encryption_spec]
  /// is not set separately.
  public var encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec {
    get {return _storage._encryptionSpec ?? Google_Cloud_Aiplatform_V1_EncryptionSpec()}
    set {_uniqueStorage()._encryptionSpec = newValue}
  }
  /// Returns true if `encryptionSpec` has been explicitly set.
  public var hasEncryptionSpec: Bool {return _storage._encryptionSpec != nil}
  /// Clears the value of `encryptionSpec`. Subsequent reads from it will return its default value.
  public mutating func clearEncryptionSpec() {_uniqueStorage()._encryptionSpec = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Specifies Vertex AI owned input data to be used for training, and
/// possibly evaluating, the Model.
public struct Google_Cloud_Aiplatform_V1_InputDataConfig: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The instructions how the input data should be split between the
  /// training, validation and test sets.
  /// If no split type is provided, the
  /// [fraction_split][google.cloud.aiplatform.v1.InputDataConfig.fraction_split]
  /// is used by default.
  public var split: Google_Cloud_Aiplatform_V1_InputDataConfig.OneOf_Split? = nil

  /// Split based on fractions defining the size of each set.
  public var fractionSplit: Google_Cloud_Aiplatform_V1_FractionSplit {
    get {
      if case .fractionSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1_FractionSplit()
    }
    set {split = .fractionSplit(newValue)}
  }

  /// Split based on the provided filters for each set.
  public var filterSplit: Google_Cloud_Aiplatform_V1_FilterSplit {
    get {
      if case .filterSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1_FilterSplit()
    }
    set {split = .filterSplit(newValue)}
  }

  /// Supported only for tabular Datasets.
  ///
  /// Split based on a predefined key.
  public var predefinedSplit: Google_Cloud_Aiplatform_V1_PredefinedSplit {
    get {
      if case .predefinedSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1_PredefinedSplit()
    }
    set {split = .predefinedSplit(newValue)}
  }

  /// Supported only for tabular Datasets.
  ///
  /// Split based on the timestamp of the input data pieces.
  public var timestampSplit: Google_Cloud_Aiplatform_V1_TimestampSplit {
    get {
      if case .timestampSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1_TimestampSplit()
    }
    set {split = .timestampSplit(newValue)}
  }

  /// Supported only for tabular Datasets.
  ///
  /// Split based on the distribution of the specified column.
  public var stratifiedSplit: Google_Cloud_Aiplatform_V1_StratifiedSplit {
    get {
      if case .stratifiedSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1_StratifiedSplit()
    }
    set {split = .stratifiedSplit(newValue)}
  }

  /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
  ///
  /// The destination of the training data to be written to.
  ///
  /// Supported destination file formats:
  ///   * For non-tabular data: "jsonl".
  ///   * For tabular data: "csv" and "bigquery".
  ///
  /// The following Vertex AI environment variables are passed to containers
  /// or python modules of the training task when this field is set:
  ///
  /// * AIP_DATA_FORMAT : Exported data format.
  /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
  /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
  /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
  public var destination: Google_Cloud_Aiplatform_V1_InputDataConfig.OneOf_Destination? = nil

  /// The Cloud Storage location where the training data is to be
  /// written to. In the given directory a new directory is created with
  /// name:
  /// `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
  /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
  /// All training input data is written into that directory.
  ///
  /// The Vertex AI environment variables representing Cloud Storage
  /// data URIs are represented in the Cloud Storage wildcard
  /// format to support sharded data. e.g.: "gs://.../training-*.jsonl"
  ///
  /// * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
  /// * AIP_TRAINING_DATA_URI =
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
  ///
  /// * AIP_VALIDATION_DATA_URI =
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
  ///
  /// * AIP_TEST_DATA_URI =
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
  public var gcsDestination: Google_Cloud_Aiplatform_V1_GcsDestination {
    get {
      if case .gcsDestination(let v)? = destination {return v}
      return Google_Cloud_Aiplatform_V1_GcsDestination()
    }
    set {destination = .gcsDestination(newValue)}
  }

  /// Only applicable to custom training with tabular Dataset with BigQuery
  /// source.
  ///
  /// The BigQuery project location where the training data is to be written
  /// to. In the given project a new dataset is created with name
  /// `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
  /// where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
  /// input data is written into that dataset. In the dataset three
  /// tables are created, `training`, `validation` and `test`.
  ///
  /// * AIP_DATA_FORMAT = "bigquery".
  /// * AIP_TRAINING_DATA_URI  =
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
  ///
  /// * AIP_VALIDATION_DATA_URI =
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
  ///
  /// * AIP_TEST_DATA_URI =
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
  public var bigqueryDestination: Google_Cloud_Aiplatform_V1_BigQueryDestination {
    get {
      if case .bigqueryDestination(let v)? = destination {return v}
      return Google_Cloud_Aiplatform_V1_BigQueryDestination()
    }
    set {destination = .bigqueryDestination(newValue)}
  }

  /// Required. The ID of the Dataset in the same Project and Location which data
  /// will be used to train the Model. The Dataset must use schema compatible
  /// with Model being trained, and what is compatible should be described in the
  /// used TrainingPipeline's [training_task_definition]
  /// [google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition].
  /// For tabular Datasets, all their data is exported to training, to pick
  /// and choose from.
  public var datasetID: String = String()

  /// Applicable only to Datasets that have DataItems and Annotations.
  ///
  /// A filter on Annotations of the Dataset. Only Annotations that both
  /// match this filter and belong to DataItems not ignored by the split method
  /// are used in respectively training, validation or test role, depending on
  /// the role of the DataItem they are on (for the auto-assigned that role is
  /// decided by Vertex AI). A filter with same syntax as the one used in
  /// [ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations]
  /// may be used, but note here it filters across all Annotations of the
  /// Dataset, and not just within a single DataItem.
  public var annotationsFilter: String = String()

  /// Applicable only to custom training with Datasets that have DataItems and
  /// Annotations.
  ///
  /// Cloud Storage URI that points to a YAML file describing the annotation
  /// schema. The schema is defined as an OpenAPI 3.0.2 [Schema
  /// Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
  /// The schema files that can be used here are found in
  /// gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the
  /// chosen schema must be consistent with
  /// [metadata][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri] of the
  /// Dataset specified by
  /// [dataset_id][google.cloud.aiplatform.v1.InputDataConfig.dataset_id].
  ///
  /// Only Annotations that both match this schema and belong to DataItems not
  /// ignored by the split method are used in respectively training, validation
  /// or test role, depending on the role of the DataItem they are on.
  ///
  /// When used in conjunction with
  /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter],
  /// the Annotations used for training are filtered by both
  /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter]
  /// and
  /// [annotation_schema_uri][google.cloud.aiplatform.v1.InputDataConfig.annotation_schema_uri].
  public var annotationSchemaUri: String = String()

  /// Only applicable to Datasets that have SavedQueries.
  ///
  /// The ID of a SavedQuery (annotation set) under the Dataset specified by
  /// [dataset_id][google.cloud.aiplatform.v1.InputDataConfig.dataset_id] used
  /// for filtering Annotations for training.
  ///
  /// Only Annotations that are associated with this SavedQuery are used in
  /// respectively training. When used in conjunction with
  /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter],
  /// the Annotations used for training are filtered by both
  /// [saved_query_id][google.cloud.aiplatform.v1.InputDataConfig.saved_query_id]
  /// and
  /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter].
  ///
  /// Only one of
  /// [saved_query_id][google.cloud.aiplatform.v1.InputDataConfig.saved_query_id]
  /// and
  /// [annotation_schema_uri][google.cloud.aiplatform.v1.InputDataConfig.annotation_schema_uri]
  /// should be specified as both of them represent the same thing: problem type.
  public var savedQueryID: String = String()

  /// Whether to persist the ML use assignment to data item system labels.
  public var persistMlUseAssignment: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The instructions how the input data should be split between the
  /// training, validation and test sets.
  /// If no split type is provided, the
  /// [fraction_split][google.cloud.aiplatform.v1.InputDataConfig.fraction_split]
  /// is used by default.
  public enum OneOf_Split: Equatable, Sendable {
    /// Split based on fractions defining the size of each set.
    case fractionSplit(Google_Cloud_Aiplatform_V1_FractionSplit)
    /// Split based on the provided filters for each set.
    case filterSplit(Google_Cloud_Aiplatform_V1_FilterSplit)
    /// Supported only for tabular Datasets.
    ///
    /// Split based on a predefined key.
    case predefinedSplit(Google_Cloud_Aiplatform_V1_PredefinedSplit)
    /// Supported only for tabular Datasets.
    ///
    /// Split based on the timestamp of the input data pieces.
    case timestampSplit(Google_Cloud_Aiplatform_V1_TimestampSplit)
    /// Supported only for tabular Datasets.
    ///
    /// Split based on the distribution of the specified column.
    case stratifiedSplit(Google_Cloud_Aiplatform_V1_StratifiedSplit)

  }

  /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
  ///
  /// The destination of the training data to be written to.
  ///
  /// Supported destination file formats:
  ///   * For non-tabular data: "jsonl".
  ///   * For tabular data: "csv" and "bigquery".
  ///
  /// The following Vertex AI environment variables are passed to containers
  /// or python modules of the training task when this field is set:
  ///
  /// * AIP_DATA_FORMAT : Exported data format.
  /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
  /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
  /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
  public enum OneOf_Destination: Equatable, Sendable {
    /// The Cloud Storage location where the training data is to be
    /// written to. In the given directory a new directory is created with
    /// name:
    /// `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
    /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
    /// All training input data is written into that directory.
    ///
    /// The Vertex AI environment variables representing Cloud Storage
    /// data URIs are represented in the Cloud Storage wildcard
    /// format to support sharded data. e.g.: "gs://.../training-*.jsonl"
    ///
    /// * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
    /// * AIP_TRAINING_DATA_URI =
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
    ///
    /// * AIP_VALIDATION_DATA_URI =
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
    ///
    /// * AIP_TEST_DATA_URI =
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
    case gcsDestination(Google_Cloud_Aiplatform_V1_GcsDestination)
    /// Only applicable to custom training with tabular Dataset with BigQuery
    /// source.
    ///
    /// The BigQuery project location where the training data is to be written
    /// to. In the given project a new dataset is created with name
    /// `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
    /// where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
    /// input data is written into that dataset. In the dataset three
    /// tables are created, `training`, `validation` and `test`.
    ///
    /// * AIP_DATA_FORMAT = "bigquery".
    /// * AIP_TRAINING_DATA_URI  =
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
    ///
    /// * AIP_VALIDATION_DATA_URI =
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
    ///
    /// * AIP_TEST_DATA_URI =
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
    case bigqueryDestination(Google_Cloud_Aiplatform_V1_BigQueryDestination)

  }

  public init() {}
}

/// Assigns the input data to training, validation, and test sets as per the
/// given fractions. Any of `training_fraction`, `validation_fraction` and
/// `test_fraction` may optionally be provided, they must sum to up to 1. If the
/// provided ones sum to less than 1, the remainder is assigned to sets as
/// decided by Vertex AI. If none of the fractions are set, by default roughly
/// 80% of data is used for training, 10% for validation, and 10% for test.
public struct Google_Cloud_Aiplatform_V1_FractionSplit: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The fraction of the input data that is to be used to train the Model.
  public var trainingFraction: Double = 0

  /// The fraction of the input data that is to be used to validate the Model.
  public var validationFraction: Double = 0

  /// The fraction of the input data that is to be used to evaluate the Model.
  public var testFraction: Double = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on the given
/// filters, data pieces not matched by any filter are ignored. Currently only
/// supported for Datasets containing DataItems.
/// If any of the filters in this message are to match nothing, then they can be
/// set as '-' (the minus sign).
///
/// Supported only for unstructured Datasets.
public struct Google_Cloud_Aiplatform_V1_FilterSplit: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to train the Model. A filter with same syntax
  /// as the one used in
  /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
  /// may be used. If a single DataItem is matched by more than one of the
  /// FilterSplit filters, then it is assigned to the first set that applies to
  /// it in the training, validation, test order.
  public var trainingFilter: String = String()

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to validate the Model. A filter with same syntax
  /// as the one used in
  /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
  /// may be used. If a single DataItem is matched by more than one of the
  /// FilterSplit filters, then it is assigned to the first set that applies to
  /// it in the training, validation, test order.
  public var validationFilter: String = String()

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to test the Model. A filter with same syntax
  /// as the one used in
  /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
  /// may be used. If a single DataItem is matched by more than one of the
  /// FilterSplit filters, then it is assigned to the first set that applies to
  /// it in the training, validation, test order.
  public var testFilter: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on the
/// value of a provided key.
///
/// Supported only for tabular Datasets.
public struct Google_Cloud_Aiplatform_V1_PredefinedSplit: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The key is a name of one of the Dataset's data columns.
  /// The value of the key (either the label's value or value in the column)
  /// must be one of {`training`, `validation`, `test`}, and it defines to which
  /// set the given piece of data is assigned. If for a piece of data the key
  /// is not present or has an invalid value, that piece is ignored by the
  /// pipeline.
  public var key: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on a
/// provided timestamps. The youngest data pieces are assigned to training set,
/// next to validation set, and the oldest to the test set.
///
/// Supported only for tabular Datasets.
public struct Google_Cloud_Aiplatform_V1_TimestampSplit: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The fraction of the input data that is to be used to train the Model.
  public var trainingFraction: Double = 0

  /// The fraction of the input data that is to be used to validate the Model.
  public var validationFraction: Double = 0

  /// The fraction of the input data that is to be used to evaluate the Model.
  public var testFraction: Double = 0

  /// Required. The key is a name of one of the Dataset's data columns.
  /// The values of the key (the values in the column) must be in RFC 3339
  /// `date-time` format, where `time-offset` = `"Z"`
  /// (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not
  /// present or has an invalid value, that piece is ignored by the pipeline.
  public var key: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to the training, validation, and test sets so that the
/// distribution of values found in the categorical column (as specified by the
/// `key` field) is mirrored within each split. The fraction values determine
/// the relative sizes of the splits.
///
/// For example, if the specified column has three values, with 50% of the rows
/// having value "A", 25% value "B", and 25% value "C", and the split fractions
/// are specified as 80/10/10, then the training set will constitute 80% of the
/// training data, with about 50% of the training set rows having the value "A"
/// for the specified column, about 25% having the value "B", and about 25%
/// having the value "C".
///
/// Only the top 500 occurring values are used; any values not in the top
/// 500 values are randomly assigned to a split. If less than three rows contain
/// a specific value, those rows are randomly assigned.
///
/// Supported only for tabular Datasets.
public struct Google_Cloud_Aiplatform_V1_StratifiedSplit: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The fraction of the input data that is to be used to train the Model.
  public var trainingFraction: Double = 0

  /// The fraction of the input data that is to be used to validate the Model.
  public var validationFraction: Double = 0

  /// The fraction of the input data that is to be used to evaluate the Model.
  public var testFraction: Double = 0

  /// Required. The key is a name of one of the Dataset's data columns.
  /// The key provided must be for a categorical column.
  public var key: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1"

extension Google_Cloud_Aiplatform_V1_TrainingPipeline: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TrainingPipeline"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .standard(proto: "input_data_config"),
    4: .standard(proto: "training_task_definition"),
    5: .standard(proto: "training_task_inputs"),
    6: .standard(proto: "training_task_metadata"),
    7: .standard(proto: "model_to_upload"),
    22: .standard(proto: "model_id"),
    21: .standard(proto: "parent_model"),
    9: .same(proto: "state"),
    10: .same(proto: "error"),
    11: .standard(proto: "create_time"),
    12: .standard(proto: "start_time"),
    13: .standard(proto: "end_time"),
    14: .standard(proto: "update_time"),
    15: .same(proto: "labels"),
    18: .standard(proto: "encryption_spec"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _displayName: String = String()
    var _inputDataConfig: Google_Cloud_Aiplatform_V1_InputDataConfig? = nil
    var _trainingTaskDefinition: String = String()
    var _trainingTaskInputs: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _trainingTaskMetadata: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _modelToUpload: Google_Cloud_Aiplatform_V1_Model? = nil
    var _modelID: String = String()
    var _parentModel: String = String()
    var _state: Google_Cloud_Aiplatform_V1_PipelineState = .unspecified
    var _error: Google_Rpc_Status? = nil
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _labels: Dictionary<String,String> = [:]
    var _encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _displayName = source._displayName
      _inputDataConfig = source._inputDataConfig
      _trainingTaskDefinition = source._trainingTaskDefinition
      _trainingTaskInputs = source._trainingTaskInputs
      _trainingTaskMetadata = source._trainingTaskMetadata
      _modelToUpload = source._modelToUpload
      _modelID = source._modelID
      _parentModel = source._parentModel
      _state = source._state
      _error = source._error
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _updateTime = source._updateTime
      _labels = source._labels
      _encryptionSpec = source._encryptionSpec
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._displayName) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._inputDataConfig) }()
        case 4: try { try decoder.decodeSingularStringField(value: &_storage._trainingTaskDefinition) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._trainingTaskInputs) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._trainingTaskMetadata) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._modelToUpload) }()
        case 9: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._updateTime) }()
        case 15: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 18: try { try decoder.decodeSingularMessageField(value: &_storage._encryptionSpec) }()
        case 21: try { try decoder.decodeSingularStringField(value: &_storage._parentModel) }()
        case 22: try { try decoder.decodeSingularStringField(value: &_storage._modelID) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if !_storage._displayName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._displayName, fieldNumber: 2)
      }
      try { if let v = _storage._inputDataConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      if !_storage._trainingTaskDefinition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._trainingTaskDefinition, fieldNumber: 4)
      }
      try { if let v = _storage._trainingTaskInputs {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._trainingTaskMetadata {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      try { if let v = _storage._modelToUpload {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 9)
      }
      try { if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      try { if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      try { if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      } }()
      try { if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      } }()
      try { if let v = _storage._updateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      } }()
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 15)
      }
      try { if let v = _storage._encryptionSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 18)
      } }()
      if !_storage._parentModel.isEmpty {
        try visitor.visitSingularStringField(value: _storage._parentModel, fieldNumber: 21)
      }
      if !_storage._modelID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._modelID, fieldNumber: 22)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_TrainingPipeline, rhs: Google_Cloud_Aiplatform_V1_TrainingPipeline) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._displayName != rhs_storage._displayName {return false}
        if _storage._inputDataConfig != rhs_storage._inputDataConfig {return false}
        if _storage._trainingTaskDefinition != rhs_storage._trainingTaskDefinition {return false}
        if _storage._trainingTaskInputs != rhs_storage._trainingTaskInputs {return false}
        if _storage._trainingTaskMetadata != rhs_storage._trainingTaskMetadata {return false}
        if _storage._modelToUpload != rhs_storage._modelToUpload {return false}
        if _storage._modelID != rhs_storage._modelID {return false}
        if _storage._parentModel != rhs_storage._parentModel {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._updateTime != rhs_storage._updateTime {return false}
        if _storage._labels != rhs_storage._labels {return false}
        if _storage._encryptionSpec != rhs_storage._encryptionSpec {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_InputDataConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InputDataConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "fraction_split"),
    3: .standard(proto: "filter_split"),
    4: .standard(proto: "predefined_split"),
    5: .standard(proto: "timestamp_split"),
    12: .standard(proto: "stratified_split"),
    8: .standard(proto: "gcs_destination"),
    10: .standard(proto: "bigquery_destination"),
    1: .standard(proto: "dataset_id"),
    6: .standard(proto: "annotations_filter"),
    9: .standard(proto: "annotation_schema_uri"),
    7: .standard(proto: "saved_query_id"),
    11: .standard(proto: "persist_ml_use_assignment"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1_FractionSplit?
        var hadOneofValue = false
        if let current = self.split {
          hadOneofValue = true
          if case .fractionSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.split = .fractionSplit(v)
        }
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1_FilterSplit?
        var hadOneofValue = false
        if let current = self.split {
          hadOneofValue = true
          if case .filterSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.split = .filterSplit(v)
        }
      }()
      case 4: try {
        var v: Google_Cloud_Aiplatform_V1_PredefinedSplit?
        var hadOneofValue = false
        if let current = self.split {
          hadOneofValue = true
          if case .predefinedSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.split = .predefinedSplit(v)
        }
      }()
      case 5: try {
        var v: Google_Cloud_Aiplatform_V1_TimestampSplit?
        var hadOneofValue = false
        if let current = self.split {
          hadOneofValue = true
          if case .timestampSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.split = .timestampSplit(v)
        }
      }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.annotationsFilter) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.savedQueryID) }()
      case 8: try {
        var v: Google_Cloud_Aiplatform_V1_GcsDestination?
        var hadOneofValue = false
        if let current = self.destination {
          hadOneofValue = true
          if case .gcsDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.destination = .gcsDestination(v)
        }
      }()
      case 9: try { try decoder.decodeSingularStringField(value: &self.annotationSchemaUri) }()
      case 10: try {
        var v: Google_Cloud_Aiplatform_V1_BigQueryDestination?
        var hadOneofValue = false
        if let current = self.destination {
          hadOneofValue = true
          if case .bigqueryDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.destination = .bigqueryDestination(v)
        }
      }()
      case 11: try { try decoder.decodeSingularBoolField(value: &self.persistMlUseAssignment) }()
      case 12: try {
        var v: Google_Cloud_Aiplatform_V1_StratifiedSplit?
        var hadOneofValue = false
        if let current = self.split {
          hadOneofValue = true
          if case .stratifiedSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.split = .stratifiedSplit(v)
        }
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 1)
    }
    switch self.split {
    case .fractionSplit?: try {
      guard case .fractionSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .filterSplit?: try {
      guard case .filterSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .predefinedSplit?: try {
      guard case .predefinedSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case .timestampSplit?: try {
      guard case .timestampSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    default: break
    }
    if !self.annotationsFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationsFilter, fieldNumber: 6)
    }
    if !self.savedQueryID.isEmpty {
      try visitor.visitSingularStringField(value: self.savedQueryID, fieldNumber: 7)
    }
    try { if case .gcsDestination(let v)? = self.destination {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    } }()
    if !self.annotationSchemaUri.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationSchemaUri, fieldNumber: 9)
    }
    try { if case .bigqueryDestination(let v)? = self.destination {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
    } }()
    if self.persistMlUseAssignment != false {
      try visitor.visitSingularBoolField(value: self.persistMlUseAssignment, fieldNumber: 11)
    }
    try { if case .stratifiedSplit(let v)? = self.split {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_InputDataConfig, rhs: Google_Cloud_Aiplatform_V1_InputDataConfig) -> Bool {
    if lhs.split != rhs.split {return false}
    if lhs.destination != rhs.destination {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.annotationsFilter != rhs.annotationsFilter {return false}
    if lhs.annotationSchemaUri != rhs.annotationSchemaUri {return false}
    if lhs.savedQueryID != rhs.savedQueryID {return false}
    if lhs.persistMlUseAssignment != rhs.persistMlUseAssignment {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_FractionSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FractionSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_fraction"),
    2: .standard(proto: "validation_fraction"),
    3: .standard(proto: "test_fraction"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.trainingFraction) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.validationFraction) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.testFraction) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.trainingFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.trainingFraction, fieldNumber: 1)
    }
    if self.validationFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.validationFraction, fieldNumber: 2)
    }
    if self.testFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.testFraction, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_FractionSplit, rhs: Google_Cloud_Aiplatform_V1_FractionSplit) -> Bool {
    if lhs.trainingFraction != rhs.trainingFraction {return false}
    if lhs.validationFraction != rhs.validationFraction {return false}
    if lhs.testFraction != rhs.testFraction {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_FilterSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FilterSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_filter"),
    2: .standard(proto: "validation_filter"),
    3: .standard(proto: "test_filter"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.trainingFilter) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.validationFilter) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.testFilter) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.trainingFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.trainingFilter, fieldNumber: 1)
    }
    if !self.validationFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.validationFilter, fieldNumber: 2)
    }
    if !self.testFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.testFilter, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_FilterSplit, rhs: Google_Cloud_Aiplatform_V1_FilterSplit) -> Bool {
    if lhs.trainingFilter != rhs.trainingFilter {return false}
    if lhs.validationFilter != rhs.validationFilter {return false}
    if lhs.testFilter != rhs.testFilter {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PredefinedSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PredefinedSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "key"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.key) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PredefinedSplit, rhs: Google_Cloud_Aiplatform_V1_PredefinedSplit) -> Bool {
    if lhs.key != rhs.key {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_TimestampSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TimestampSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_fraction"),
    2: .standard(proto: "validation_fraction"),
    3: .standard(proto: "test_fraction"),
    4: .same(proto: "key"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.trainingFraction) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.validationFraction) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.testFraction) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.key) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.trainingFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.trainingFraction, fieldNumber: 1)
    }
    if self.validationFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.validationFraction, fieldNumber: 2)
    }
    if self.testFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.testFraction, fieldNumber: 3)
    }
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_TimestampSplit, rhs: Google_Cloud_Aiplatform_V1_TimestampSplit) -> Bool {
    if lhs.trainingFraction != rhs.trainingFraction {return false}
    if lhs.validationFraction != rhs.validationFraction {return false}
    if lhs.testFraction != rhs.testFraction {return false}
    if lhs.key != rhs.key {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_StratifiedSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StratifiedSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_fraction"),
    2: .standard(proto: "validation_fraction"),
    3: .standard(proto: "test_fraction"),
    4: .same(proto: "key"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.trainingFraction) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.validationFraction) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.testFraction) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.key) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.trainingFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.trainingFraction, fieldNumber: 1)
    }
    if self.validationFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.validationFraction, fieldNumber: 2)
    }
    if self.testFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.testFraction, fieldNumber: 3)
    }
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_StratifiedSplit, rhs: Google_Cloud_Aiplatform_V1_StratifiedSplit) -> Bool {
    if lhs.trainingFraction != rhs.trainingFraction {return false}
    if lhs.validationFraction != rhs.validationFraction {return false}
    if lhs.testFraction != rhs.testFraction {return false}
    if lhs.key != rhs.key {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
