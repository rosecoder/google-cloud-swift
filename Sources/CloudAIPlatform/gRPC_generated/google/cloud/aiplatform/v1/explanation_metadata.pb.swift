// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1/explanation_metadata.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Metadata describing the Model's input and output for explanation.
public struct Google_Cloud_Aiplatform_V1_ExplanationMetadata: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Map from feature names to feature input metadata. Keys are the
  /// name of the features. Values are the specification of the feature.
  ///
  /// An empty InputMetadata is valid. It describes a text feature which has the
  /// name specified as the key in
  /// [ExplanationMetadata.inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
  /// The baseline of the empty feature is chosen by Vertex AI.
  ///
  /// For Vertex AI-provided Tensorflow images, the key can be any friendly
  /// name of the feature. Once specified,
  /// [featureAttributions][google.cloud.aiplatform.v1.Attribution.feature_attributions]
  /// are keyed by this key (if not grouped with another feature).
  ///
  /// For custom images, the key must match with the key in
  /// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances].
  public var inputs: Dictionary<String,Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata> = [:]

  /// Required. Map from output names to output metadata.
  ///
  /// For Vertex AI-provided Tensorflow images, keys can be any user defined
  /// string that consists of any UTF-8 characters.
  ///
  /// For custom images, keys are the name of the output field in the prediction
  /// to be explained.
  ///
  /// Currently only one key is allowed.
  public var outputs: Dictionary<String,Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata> = [:]

  /// Points to a YAML file stored on Google Cloud Storage describing the format
  /// of the [feature
  /// attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
  /// The schema is defined as an OpenAPI 3.0.2 [Schema
  /// Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
  /// AutoML tabular Models always have this field populated by Vertex AI.
  /// Note: The URI given on output may be different, including the URI scheme,
  /// than the one given on input. The output URI will point to a location where
  /// the user only has a read access.
  public var featureAttributionsSchemaUri: String = String()

  /// Name of the source to generate embeddings for example based explanations.
  public var latentSpaceSource: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Metadata of the input of a feature.
  ///
  /// Fields other than
  /// [InputMetadata.input_baselines][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.input_baselines]
  /// are applicable only for Models that are using Vertex AI-provided images for
  /// Tensorflow.
  public struct InputMetadata: @unchecked Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Baseline inputs for this feature.
    ///
    /// If no baseline is specified, Vertex AI chooses the baseline for this
    /// feature. If multiple baselines are specified, Vertex AI returns the
    /// average attributions across them in
    /// [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
    ///
    /// For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape
    /// of each baseline must match the shape of the input tensor. If a scalar is
    /// provided, we broadcast to the same shape as the input tensor.
    ///
    /// For custom images, the element of the baselines must be in the same
    /// format as the feature's input in the
    /// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances][]. The
    /// schema of any single instance may be specified via Endpoint's
    /// DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
    public var inputBaselines: [SwiftProtobuf.Google_Protobuf_Value] {
      get {return _storage._inputBaselines}
      set {_uniqueStorage()._inputBaselines = newValue}
    }

    /// Name of the input tensor for this feature. Required and is only
    /// applicable to Vertex AI-provided images for Tensorflow.
    public var inputTensorName: String {
      get {return _storage._inputTensorName}
      set {_uniqueStorage()._inputTensorName = newValue}
    }

    /// Defines how the feature is encoded into the input tensor. Defaults to
    /// IDENTITY.
    public var encoding: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Encoding {
      get {return _storage._encoding}
      set {_uniqueStorage()._encoding = newValue}
    }

    /// Modality of the feature. Valid values are: numeric, image. Defaults to
    /// numeric.
    public var modality: String {
      get {return _storage._modality}
      set {_uniqueStorage()._modality = newValue}
    }

    /// The domain details of the input feature value. Like min/max, original
    /// mean or standard deviation if normalized.
    public var featureValueDomain: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain {
      get {return _storage._featureValueDomain ?? Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain()}
      set {_uniqueStorage()._featureValueDomain = newValue}
    }
    /// Returns true if `featureValueDomain` has been explicitly set.
    public var hasFeatureValueDomain: Bool {return _storage._featureValueDomain != nil}
    /// Clears the value of `featureValueDomain`. Subsequent reads from it will return its default value.
    public mutating func clearFeatureValueDomain() {_uniqueStorage()._featureValueDomain = nil}

    /// Specifies the index of the values of the input tensor.
    /// Required when the input tensor is a sparse representation. Refer to
    /// Tensorflow documentation for more details:
    /// https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
    public var indicesTensorName: String {
      get {return _storage._indicesTensorName}
      set {_uniqueStorage()._indicesTensorName = newValue}
    }

    /// Specifies the shape of the values of the input if the input is a sparse
    /// representation. Refer to Tensorflow documentation for more details:
    /// https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
    public var denseShapeTensorName: String {
      get {return _storage._denseShapeTensorName}
      set {_uniqueStorage()._denseShapeTensorName = newValue}
    }

    /// A list of feature names for each index in the input tensor.
    /// Required when the input
    /// [InputMetadata.encoding][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoding]
    /// is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
    public var indexFeatureMapping: [String] {
      get {return _storage._indexFeatureMapping}
      set {_uniqueStorage()._indexFeatureMapping = newValue}
    }

    /// Encoded tensor is a transformation of the input tensor. Must be provided
    /// if choosing
    /// [Integrated Gradients
    /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution]
    /// or [XRAI
    /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution]
    /// and the input tensor is not differentiable.
    ///
    /// An encoded tensor is generated if the input tensor is encoded by a lookup
    /// table.
    public var encodedTensorName: String {
      get {return _storage._encodedTensorName}
      set {_uniqueStorage()._encodedTensorName = newValue}
    }

    /// A list of baselines for the encoded tensor.
    ///
    /// The shape of each baseline should match the shape of the encoded tensor.
    /// If a scalar is provided, Vertex AI broadcasts to the same shape as the
    /// encoded tensor.
    public var encodedBaselines: [SwiftProtobuf.Google_Protobuf_Value] {
      get {return _storage._encodedBaselines}
      set {_uniqueStorage()._encodedBaselines = newValue}
    }

    /// Visualization configurations for image explanation.
    public var visualization: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization {
      get {return _storage._visualization ?? Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization()}
      set {_uniqueStorage()._visualization = newValue}
    }
    /// Returns true if `visualization` has been explicitly set.
    public var hasVisualization: Bool {return _storage._visualization != nil}
    /// Clears the value of `visualization`. Subsequent reads from it will return its default value.
    public mutating func clearVisualization() {_uniqueStorage()._visualization = nil}

    /// Name of the group that the input belongs to. Features with the same group
    /// name will be treated as one feature when computing attributions. Features
    /// grouped together can have different shapes in value. If provided, there
    /// will be one single attribution generated in
    /// [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions],
    /// keyed by the group name.
    public var groupName: String {
      get {return _storage._groupName}
      set {_uniqueStorage()._groupName = newValue}
    }

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Defines how a feature is encoded. Defaults to IDENTITY.
    public enum Encoding: SwiftProtobuf.Enum, Swift.CaseIterable {
      public typealias RawValue = Int

      /// Default value. This is the same as IDENTITY.
      case unspecified // = 0

      /// The tensor represents one feature.
      case identity // = 1

      /// The tensor represents a bag of features where each index maps to
      /// a feature.
      /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
      /// must be provided for this encoding. For example:
      /// ```
      /// input = [27, 6.0, 150]
      /// index_feature_mapping = ["age", "height", "weight"]
      /// ```
      case bagOfFeatures // = 2

      /// The tensor represents a bag of features where each index maps to a
      /// feature. Zero values in the tensor indicates feature being
      /// non-existent.
      /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
      /// must be provided for this encoding. For example:
      /// ```
      /// input = [2, 0, 5, 0, 1]
      /// index_feature_mapping = ["a", "b", "c", "d", "e"]
      /// ```
      case bagOfFeaturesSparse // = 3

      /// The tensor is a list of binaries representing whether a feature exists
      /// or not (1 indicates existence).
      /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
      /// must be provided for this encoding. For example:
      /// ```
      /// input = [1, 0, 1, 0, 1]
      /// index_feature_mapping = ["a", "b", "c", "d", "e"]
      /// ```
      case indicator // = 4

      /// The tensor is encoded into a 1-dimensional array represented by an
      /// encoded tensor.
      /// [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
      /// must be provided for this encoding. For example:
      /// ```
      /// input = ["This", "is", "a", "test", "."]
      /// encoded = [0.1, 0.2, 0.3, 0.4, 0.5]
      /// ```
      case combinedEmbedding // = 5

      /// Select this encoding when the input tensor is encoded into a
      /// 2-dimensional array represented by an encoded tensor.
      /// [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
      /// must be provided for this encoding. The first dimension of the encoded
      /// tensor's shape is the same as the input tensor's shape. For example:
      /// ```
      /// input = ["This", "is", "a", "test", "."]
      /// encoded = [[0.1, 0.2, 0.3, 0.4, 0.5],
      ///            [0.2, 0.1, 0.4, 0.3, 0.5],
      ///            [0.5, 0.1, 0.3, 0.5, 0.4],
      ///            [0.5, 0.3, 0.1, 0.2, 0.4],
      ///            [0.4, 0.3, 0.2, 0.5, 0.1]]
      /// ```
      case concatEmbedding // = 6
      case UNRECOGNIZED(Int)

      public init() {
        self = .unspecified
      }

      public init?(rawValue: Int) {
        switch rawValue {
        case 0: self = .unspecified
        case 1: self = .identity
        case 2: self = .bagOfFeatures
        case 3: self = .bagOfFeaturesSparse
        case 4: self = .indicator
        case 5: self = .combinedEmbedding
        case 6: self = .concatEmbedding
        default: self = .UNRECOGNIZED(rawValue)
        }
      }

      public var rawValue: Int {
        switch self {
        case .unspecified: return 0
        case .identity: return 1
        case .bagOfFeatures: return 2
        case .bagOfFeaturesSparse: return 3
        case .indicator: return 4
        case .combinedEmbedding: return 5
        case .concatEmbedding: return 6
        case .UNRECOGNIZED(let i): return i
        }
      }

      // The compiler won't synthesize support with the UNRECOGNIZED case.
      public static let allCases: [Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Encoding] = [
        .unspecified,
        .identity,
        .bagOfFeatures,
        .bagOfFeaturesSparse,
        .indicator,
        .combinedEmbedding,
        .concatEmbedding,
      ]

    }

    /// Domain details of the input feature value. Provides numeric information
    /// about the feature, such as its range (min, max). If the feature has been
    /// pre-processed, for example with z-scoring, then it provides information
    /// about how to recover the original feature. For example, if the input
    /// feature is an image and it has been pre-processed to obtain 0-mean and
    /// stddev = 1 values, then original_mean, and original_stddev refer to the
    /// mean and stddev of the original feature (e.g. image tensor) from which
    /// input feature (with mean = 0 and stddev = 1) was obtained.
    public struct FeatureValueDomain: Sendable {
      // SwiftProtobuf.Message conformance is added in an extension below. See the
      // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
      // methods supported on all messages.

      /// The minimum permissible value for this feature.
      public var minValue: Float = 0

      /// The maximum permissible value for this feature.
      public var maxValue: Float = 0

      /// If this input feature has been normalized to a mean value of 0,
      /// the original_mean specifies the mean value of the domain prior to
      /// normalization.
      public var originalMean: Float = 0

      /// If this input feature has been normalized to a standard deviation of
      /// 1.0, the original_stddev specifies the standard deviation of the domain
      /// prior to normalization.
      public var originalStddev: Float = 0

      public var unknownFields = SwiftProtobuf.UnknownStorage()

      public init() {}
    }

    /// Visualization configurations for image explanation.
    public struct Visualization: Sendable {
      // SwiftProtobuf.Message conformance is added in an extension below. See the
      // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
      // methods supported on all messages.

      /// Type of the image visualization. Only applicable to
      /// [Integrated Gradients
      /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
      /// OUTLINES shows regions of attribution, while PIXELS shows per-pixel
      /// attribution. Defaults to OUTLINES.
      public var type: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.TypeEnum = .unspecified

      /// Whether to only highlight pixels with positive contributions, negative
      /// or both. Defaults to POSITIVE.
      public var polarity: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.Polarity = .unspecified

      /// The color scheme used for the highlighted areas.
      ///
      /// Defaults to PINK_GREEN for
      /// [Integrated Gradients
      /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution],
      /// which shows positive attributions in green and negative in pink.
      ///
      /// Defaults to VIRIDIS for
      /// [XRAI
      /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution],
      /// which highlights the most influential regions in yellow and the least
      /// influential in blue.
      public var colorMap: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.ColorMap = .unspecified

      /// Excludes attributions above the specified percentile from the
      /// highlighted areas. Using the clip_percent_upperbound and
      /// clip_percent_lowerbound together can be useful for filtering out noise
      /// and making it easier to see areas of strong attribution. Defaults to
      /// 99.9.
      public var clipPercentUpperbound: Float = 0

      /// Excludes attributions below the specified percentile, from the
      /// highlighted areas. Defaults to 62.
      public var clipPercentLowerbound: Float = 0

      /// How the original image is displayed in the visualization.
      /// Adjusting the overlay can help increase visual clarity if the original
      /// image makes it difficult to view the visualization. Defaults to NONE.
      public var overlayType: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.OverlayType = .unspecified

      public var unknownFields = SwiftProtobuf.UnknownStorage()

      /// Type of the image visualization. Only applicable to
      /// [Integrated Gradients
      /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
      public enum TypeEnum: SwiftProtobuf.Enum, Swift.CaseIterable {
        public typealias RawValue = Int

        /// Should not be used.
        case unspecified // = 0

        /// Shows which pixel contributed to the image prediction.
        case pixels // = 1

        /// Shows which region contributed to the image prediction by outlining
        /// the region.
        case outlines // = 2
        case UNRECOGNIZED(Int)

        public init() {
          self = .unspecified
        }

        public init?(rawValue: Int) {
          switch rawValue {
          case 0: self = .unspecified
          case 1: self = .pixels
          case 2: self = .outlines
          default: self = .UNRECOGNIZED(rawValue)
          }
        }

        public var rawValue: Int {
          switch self {
          case .unspecified: return 0
          case .pixels: return 1
          case .outlines: return 2
          case .UNRECOGNIZED(let i): return i
          }
        }

        // The compiler won't synthesize support with the UNRECOGNIZED case.
        public static let allCases: [Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.TypeEnum] = [
          .unspecified,
          .pixels,
          .outlines,
        ]

      }

      /// Whether to only highlight pixels with positive contributions, negative
      /// or both. Defaults to POSITIVE.
      public enum Polarity: SwiftProtobuf.Enum, Swift.CaseIterable {
        public typealias RawValue = Int

        /// Default value. This is the same as POSITIVE.
        case unspecified // = 0

        /// Highlights the pixels/outlines that were most influential to the
        /// model's prediction.
        case positive // = 1

        /// Setting polarity to negative highlights areas that does not lead to
        /// the models's current prediction.
        case negative // = 2

        /// Shows both positive and negative attributions.
        case both // = 3
        case UNRECOGNIZED(Int)

        public init() {
          self = .unspecified
        }

        public init?(rawValue: Int) {
          switch rawValue {
          case 0: self = .unspecified
          case 1: self = .positive
          case 2: self = .negative
          case 3: self = .both
          default: self = .UNRECOGNIZED(rawValue)
          }
        }

        public var rawValue: Int {
          switch self {
          case .unspecified: return 0
          case .positive: return 1
          case .negative: return 2
          case .both: return 3
          case .UNRECOGNIZED(let i): return i
          }
        }

        // The compiler won't synthesize support with the UNRECOGNIZED case.
        public static let allCases: [Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.Polarity] = [
          .unspecified,
          .positive,
          .negative,
          .both,
        ]

      }

      /// The color scheme used for highlighting areas.
      public enum ColorMap: SwiftProtobuf.Enum, Swift.CaseIterable {
        public typealias RawValue = Int

        /// Should not be used.
        case unspecified // = 0

        /// Positive: green. Negative: pink.
        case pinkGreen // = 1

        /// Viridis color map: A perceptually uniform color mapping which is
        /// easier to see by those with colorblindness and progresses from yellow
        /// to green to blue. Positive: yellow. Negative: blue.
        case viridis // = 2

        /// Positive: red. Negative: red.
        case red // = 3

        /// Positive: green. Negative: green.
        case green // = 4

        /// Positive: green. Negative: red.
        case redGreen // = 6

        /// PiYG palette.
        case pinkWhiteGreen // = 5
        case UNRECOGNIZED(Int)

        public init() {
          self = .unspecified
        }

        public init?(rawValue: Int) {
          switch rawValue {
          case 0: self = .unspecified
          case 1: self = .pinkGreen
          case 2: self = .viridis
          case 3: self = .red
          case 4: self = .green
          case 5: self = .pinkWhiteGreen
          case 6: self = .redGreen
          default: self = .UNRECOGNIZED(rawValue)
          }
        }

        public var rawValue: Int {
          switch self {
          case .unspecified: return 0
          case .pinkGreen: return 1
          case .viridis: return 2
          case .red: return 3
          case .green: return 4
          case .pinkWhiteGreen: return 5
          case .redGreen: return 6
          case .UNRECOGNIZED(let i): return i
          }
        }

        // The compiler won't synthesize support with the UNRECOGNIZED case.
        public static let allCases: [Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.ColorMap] = [
          .unspecified,
          .pinkGreen,
          .viridis,
          .red,
          .green,
          .redGreen,
          .pinkWhiteGreen,
        ]

      }

      /// How the original image is displayed in the visualization.
      public enum OverlayType: SwiftProtobuf.Enum, Swift.CaseIterable {
        public typealias RawValue = Int

        /// Default value. This is the same as NONE.
        case unspecified // = 0

        /// No overlay.
        case none // = 1

        /// The attributions are shown on top of the original image.
        case original // = 2

        /// The attributions are shown on top of grayscaled version of the
        /// original image.
        case grayscale // = 3

        /// The attributions are used as a mask to reveal predictive parts of
        /// the image and hide the un-predictive parts.
        case maskBlack // = 4
        case UNRECOGNIZED(Int)

        public init() {
          self = .unspecified
        }

        public init?(rawValue: Int) {
          switch rawValue {
          case 0: self = .unspecified
          case 1: self = .none
          case 2: self = .original
          case 3: self = .grayscale
          case 4: self = .maskBlack
          default: self = .UNRECOGNIZED(rawValue)
          }
        }

        public var rawValue: Int {
          switch self {
          case .unspecified: return 0
          case .none: return 1
          case .original: return 2
          case .grayscale: return 3
          case .maskBlack: return 4
          case .UNRECOGNIZED(let i): return i
          }
        }

        // The compiler won't synthesize support with the UNRECOGNIZED case.
        public static let allCases: [Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.OverlayType] = [
          .unspecified,
          .none,
          .original,
          .grayscale,
          .maskBlack,
        ]

      }

      public init() {}
    }

    public init() {}

    fileprivate var _storage = _StorageClass.defaultInstance
  }

  /// Metadata of the prediction output to be explained.
  public struct OutputMetadata: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Defines how to map
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// to
    /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name].
    ///
    /// If neither of the fields are specified,
    /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
    /// will not be populated.
    public var displayNameMapping: Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata.OneOf_DisplayNameMapping? = nil

    /// Static mapping between the index and display name.
    ///
    /// Use this if the outputs are a deterministic n-dimensional array, e.g. a
    /// list of scores of all the classes in a pre-defined order for a
    /// multi-classification Model. It's not feasible if the outputs are
    /// non-deterministic, e.g. the Model produces top-k classes or sort the
    /// outputs by their values.
    ///
    /// The shape of the value must be an n-dimensional array of strings. The
    /// number of dimensions must match that of the outputs to be explained.
    /// The
    /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
    /// is populated by locating in the mapping with
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index].
    public var indexDisplayNameMapping: SwiftProtobuf.Google_Protobuf_Value {
      get {
        if case .indexDisplayNameMapping(let v)? = displayNameMapping {return v}
        return SwiftProtobuf.Google_Protobuf_Value()
      }
      set {displayNameMapping = .indexDisplayNameMapping(newValue)}
    }

    /// Specify a field name in the prediction to look for the display name.
    ///
    /// Use this if the prediction contains the display names for the outputs.
    ///
    /// The display names in the prediction must have the same shape of the
    /// outputs, so that it can be located by
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// for a specific output.
    public var displayNameMappingKey: String {
      get {
        if case .displayNameMappingKey(let v)? = displayNameMapping {return v}
        return String()
      }
      set {displayNameMapping = .displayNameMappingKey(newValue)}
    }

    /// Name of the output tensor. Required and is only applicable to Vertex
    /// AI provided images for Tensorflow.
    public var outputTensorName: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Defines how to map
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// to
    /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name].
    ///
    /// If neither of the fields are specified,
    /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
    /// will not be populated.
    public enum OneOf_DisplayNameMapping: Equatable, Sendable {
      /// Static mapping between the index and display name.
      ///
      /// Use this if the outputs are a deterministic n-dimensional array, e.g. a
      /// list of scores of all the classes in a pre-defined order for a
      /// multi-classification Model. It's not feasible if the outputs are
      /// non-deterministic, e.g. the Model produces top-k classes or sort the
      /// outputs by their values.
      ///
      /// The shape of the value must be an n-dimensional array of strings. The
      /// number of dimensions must match that of the outputs to be explained.
      /// The
      /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
      /// is populated by locating in the mapping with
      /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index].
      case indexDisplayNameMapping(SwiftProtobuf.Google_Protobuf_Value)
      /// Specify a field name in the prediction to look for the display name.
      ///
      /// Use this if the prediction contains the display names for the outputs.
      ///
      /// The display names in the prediction must have the same shape of the
      /// outputs, so that it can be located by
      /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
      /// for a specific output.
      case displayNameMappingKey(String)

    }

    public init() {}
  }

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1"

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplanationMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "inputs"),
    2: .same(proto: "outputs"),
    3: .standard(proto: "feature_attributions_schema_uri"),
    5: .standard(proto: "latent_space_source"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata>.self, value: &self.inputs) }()
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata>.self, value: &self.outputs) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.featureAttributionsSchemaUri) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.latentSpaceSource) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputs.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata>.self, value: self.inputs, fieldNumber: 1)
    }
    if !self.outputs.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata>.self, value: self.outputs, fieldNumber: 2)
    }
    if !self.featureAttributionsSchemaUri.isEmpty {
      try visitor.visitSingularStringField(value: self.featureAttributionsSchemaUri, fieldNumber: 3)
    }
    if !self.latentSpaceSource.isEmpty {
      try visitor.visitSingularStringField(value: self.latentSpaceSource, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata, rhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata) -> Bool {
    if lhs.inputs != rhs.inputs {return false}
    if lhs.outputs != rhs.outputs {return false}
    if lhs.featureAttributionsSchemaUri != rhs.featureAttributionsSchemaUri {return false}
    if lhs.latentSpaceSource != rhs.latentSpaceSource {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_ExplanationMetadata.protoMessageName + ".InputMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_baselines"),
    2: .standard(proto: "input_tensor_name"),
    3: .same(proto: "encoding"),
    4: .same(proto: "modality"),
    5: .standard(proto: "feature_value_domain"),
    6: .standard(proto: "indices_tensor_name"),
    7: .standard(proto: "dense_shape_tensor_name"),
    8: .standard(proto: "index_feature_mapping"),
    9: .standard(proto: "encoded_tensor_name"),
    10: .standard(proto: "encoded_baselines"),
    11: .same(proto: "visualization"),
    12: .standard(proto: "group_name"),
  ]

  fileprivate class _StorageClass {
    var _inputBaselines: [SwiftProtobuf.Google_Protobuf_Value] = []
    var _inputTensorName: String = String()
    var _encoding: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Encoding = .unspecified
    var _modality: String = String()
    var _featureValueDomain: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain? = nil
    var _indicesTensorName: String = String()
    var _denseShapeTensorName: String = String()
    var _indexFeatureMapping: [String] = []
    var _encodedTensorName: String = String()
    var _encodedBaselines: [SwiftProtobuf.Google_Protobuf_Value] = []
    var _visualization: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization? = nil
    var _groupName: String = String()

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _inputBaselines = source._inputBaselines
      _inputTensorName = source._inputTensorName
      _encoding = source._encoding
      _modality = source._modality
      _featureValueDomain = source._featureValueDomain
      _indicesTensorName = source._indicesTensorName
      _denseShapeTensorName = source._denseShapeTensorName
      _indexFeatureMapping = source._indexFeatureMapping
      _encodedTensorName = source._encodedTensorName
      _encodedBaselines = source._encodedBaselines
      _visualization = source._visualization
      _groupName = source._groupName
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedMessageField(value: &_storage._inputBaselines) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._inputTensorName) }()
        case 3: try { try decoder.decodeSingularEnumField(value: &_storage._encoding) }()
        case 4: try { try decoder.decodeSingularStringField(value: &_storage._modality) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._featureValueDomain) }()
        case 6: try { try decoder.decodeSingularStringField(value: &_storage._indicesTensorName) }()
        case 7: try { try decoder.decodeSingularStringField(value: &_storage._denseShapeTensorName) }()
        case 8: try { try decoder.decodeRepeatedStringField(value: &_storage._indexFeatureMapping) }()
        case 9: try { try decoder.decodeSingularStringField(value: &_storage._encodedTensorName) }()
        case 10: try { try decoder.decodeRepeatedMessageField(value: &_storage._encodedBaselines) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._visualization) }()
        case 12: try { try decoder.decodeSingularStringField(value: &_storage._groupName) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._inputBaselines.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._inputBaselines, fieldNumber: 1)
      }
      if !_storage._inputTensorName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._inputTensorName, fieldNumber: 2)
      }
      if _storage._encoding != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._encoding, fieldNumber: 3)
      }
      if !_storage._modality.isEmpty {
        try visitor.visitSingularStringField(value: _storage._modality, fieldNumber: 4)
      }
      try { if let v = _storage._featureValueDomain {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      if !_storage._indicesTensorName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._indicesTensorName, fieldNumber: 6)
      }
      if !_storage._denseShapeTensorName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._denseShapeTensorName, fieldNumber: 7)
      }
      if !_storage._indexFeatureMapping.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._indexFeatureMapping, fieldNumber: 8)
      }
      if !_storage._encodedTensorName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._encodedTensorName, fieldNumber: 9)
      }
      if !_storage._encodedBaselines.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._encodedBaselines, fieldNumber: 10)
      }
      try { if let v = _storage._visualization {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      if !_storage._groupName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._groupName, fieldNumber: 12)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata, rhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._inputBaselines != rhs_storage._inputBaselines {return false}
        if _storage._inputTensorName != rhs_storage._inputTensorName {return false}
        if _storage._encoding != rhs_storage._encoding {return false}
        if _storage._modality != rhs_storage._modality {return false}
        if _storage._featureValueDomain != rhs_storage._featureValueDomain {return false}
        if _storage._indicesTensorName != rhs_storage._indicesTensorName {return false}
        if _storage._denseShapeTensorName != rhs_storage._denseShapeTensorName {return false}
        if _storage._indexFeatureMapping != rhs_storage._indexFeatureMapping {return false}
        if _storage._encodedTensorName != rhs_storage._encodedTensorName {return false}
        if _storage._encodedBaselines != rhs_storage._encodedBaselines {return false}
        if _storage._visualization != rhs_storage._visualization {return false}
        if _storage._groupName != rhs_storage._groupName {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Encoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "IDENTITY"),
    2: .same(proto: "BAG_OF_FEATURES"),
    3: .same(proto: "BAG_OF_FEATURES_SPARSE"),
    4: .same(proto: "INDICATOR"),
    5: .same(proto: "COMBINED_EMBEDDING"),
    6: .same(proto: "CONCAT_EMBEDDING"),
  ]
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.protoMessageName + ".FeatureValueDomain"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "min_value"),
    2: .standard(proto: "max_value"),
    3: .standard(proto: "original_mean"),
    4: .standard(proto: "original_stddev"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.minValue) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.maxValue) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.originalMean) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.originalStddev) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.minValue.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.minValue, fieldNumber: 1)
    }
    if self.maxValue.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.maxValue, fieldNumber: 2)
    }
    if self.originalMean.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.originalMean, fieldNumber: 3)
    }
    if self.originalStddev.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.originalStddev, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain, rhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.FeatureValueDomain) -> Bool {
    if lhs.minValue != rhs.minValue {return false}
    if lhs.maxValue != rhs.maxValue {return false}
    if lhs.originalMean != rhs.originalMean {return false}
    if lhs.originalStddev != rhs.originalStddev {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.protoMessageName + ".Visualization"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "type"),
    2: .same(proto: "polarity"),
    3: .standard(proto: "color_map"),
    4: .standard(proto: "clip_percent_upperbound"),
    5: .standard(proto: "clip_percent_lowerbound"),
    6: .standard(proto: "overlay_type"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.type) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.polarity) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.colorMap) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.clipPercentUpperbound) }()
      case 5: try { try decoder.decodeSingularFloatField(value: &self.clipPercentLowerbound) }()
      case 6: try { try decoder.decodeSingularEnumField(value: &self.overlayType) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.type != .unspecified {
      try visitor.visitSingularEnumField(value: self.type, fieldNumber: 1)
    }
    if self.polarity != .unspecified {
      try visitor.visitSingularEnumField(value: self.polarity, fieldNumber: 2)
    }
    if self.colorMap != .unspecified {
      try visitor.visitSingularEnumField(value: self.colorMap, fieldNumber: 3)
    }
    if self.clipPercentUpperbound.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.clipPercentUpperbound, fieldNumber: 4)
    }
    if self.clipPercentLowerbound.bitPattern != 0 {
      try visitor.visitSingularFloatField(value: self.clipPercentLowerbound, fieldNumber: 5)
    }
    if self.overlayType != .unspecified {
      try visitor.visitSingularEnumField(value: self.overlayType, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization, rhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization) -> Bool {
    if lhs.type != rhs.type {return false}
    if lhs.polarity != rhs.polarity {return false}
    if lhs.colorMap != rhs.colorMap {return false}
    if lhs.clipPercentUpperbound != rhs.clipPercentUpperbound {return false}
    if lhs.clipPercentLowerbound != rhs.clipPercentLowerbound {return false}
    if lhs.overlayType != rhs.overlayType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.TypeEnum: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "TYPE_UNSPECIFIED"),
    1: .same(proto: "PIXELS"),
    2: .same(proto: "OUTLINES"),
  ]
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.Polarity: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "POLARITY_UNSPECIFIED"),
    1: .same(proto: "POSITIVE"),
    2: .same(proto: "NEGATIVE"),
    3: .same(proto: "BOTH"),
  ]
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.ColorMap: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "COLOR_MAP_UNSPECIFIED"),
    1: .same(proto: "PINK_GREEN"),
    2: .same(proto: "VIRIDIS"),
    3: .same(proto: "RED"),
    4: .same(proto: "GREEN"),
    5: .same(proto: "PINK_WHITE_GREEN"),
    6: .same(proto: "RED_GREEN"),
  ]
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.InputMetadata.Visualization.OverlayType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "OVERLAY_TYPE_UNSPECIFIED"),
    1: .same(proto: "NONE"),
    2: .same(proto: "ORIGINAL"),
    3: .same(proto: "GRAYSCALE"),
    4: .same(proto: "MASK_BLACK"),
  ]
}

extension Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_ExplanationMetadata.protoMessageName + ".OutputMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "index_display_name_mapping"),
    2: .standard(proto: "display_name_mapping_key"),
    3: .standard(proto: "output_tensor_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: SwiftProtobuf.Google_Protobuf_Value?
        var hadOneofValue = false
        if let current = self.displayNameMapping {
          hadOneofValue = true
          if case .indexDisplayNameMapping(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.displayNameMapping = .indexDisplayNameMapping(v)
        }
      }()
      case 2: try {
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {
          if self.displayNameMapping != nil {try decoder.handleConflictingOneOf()}
          self.displayNameMapping = .displayNameMappingKey(v)
        }
      }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.outputTensorName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    switch self.displayNameMapping {
    case .indexDisplayNameMapping?: try {
      guard case .indexDisplayNameMapping(let v)? = self.displayNameMapping else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .displayNameMappingKey?: try {
      guard case .displayNameMappingKey(let v)? = self.displayNameMapping else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if !self.outputTensorName.isEmpty {
      try visitor.visitSingularStringField(value: self.outputTensorName, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata, rhs: Google_Cloud_Aiplatform_V1_ExplanationMetadata.OutputMetadata) -> Bool {
    if lhs.displayNameMapping != rhs.displayNameMapping {return false}
    if lhs.outputTensorName != rhs.outputTensorName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
