// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1/batch_prediction_job.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// A job that uses a
/// [Model][google.cloud.aiplatform.v1.BatchPredictionJob.model] to produce
/// predictions on multiple [input
/// instances][google.cloud.aiplatform.v1.BatchPredictionJob.input_config]. If
/// predictions for significant portion of the instances fail, the job may finish
/// without attempting predictions for all remaining instances.
public struct Google_Cloud_Aiplatform_V1_BatchPredictionJob: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Resource name of the BatchPredictionJob.
  public var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// Required. The user-defined name of this BatchPredictionJob.
  public var displayName: String {
    get {return _storage._displayName}
    set {_uniqueStorage()._displayName = newValue}
  }

  /// The name of the Model resource that produces the predictions via this job,
  /// must share the same ancestor Location.
  /// Starting this job has no impact on any existing deployments of the Model
  /// and their resources.
  /// Exactly one of model and unmanaged_container_model must be set.
  ///
  /// The model resource name may contain version id or version alias to specify
  /// the version.
  ///  Example: `projects/{project}/locations/{location}/models/{model}@2`
  ///              or
  ///            `projects/{project}/locations/{location}/models/{model}@golden`
  /// if no version is specified, the default version will be deployed.
  ///
  /// The model resource could also be a publisher model.
  ///  Example: `publishers/{publisher}/models/{model}`
  ///              or
  ///           `projects/{project}/locations/{location}/publishers/{publisher}/models/{model}`
  public var model: String {
    get {return _storage._model}
    set {_uniqueStorage()._model = newValue}
  }

  /// Output only. The version ID of the Model that produces the predictions via
  /// this job.
  public var modelVersionID: String {
    get {return _storage._modelVersionID}
    set {_uniqueStorage()._modelVersionID = newValue}
  }

  /// Contains model information necessary to perform batch prediction without
  /// requiring uploading to model registry.
  /// Exactly one of model and unmanaged_container_model must be set.
  public var unmanagedContainerModel: Google_Cloud_Aiplatform_V1_UnmanagedContainerModel {
    get {return _storage._unmanagedContainerModel ?? Google_Cloud_Aiplatform_V1_UnmanagedContainerModel()}
    set {_uniqueStorage()._unmanagedContainerModel = newValue}
  }
  /// Returns true if `unmanagedContainerModel` has been explicitly set.
  public var hasUnmanagedContainerModel: Bool {return _storage._unmanagedContainerModel != nil}
  /// Clears the value of `unmanagedContainerModel`. Subsequent reads from it will return its default value.
  public mutating func clearUnmanagedContainerModel() {_uniqueStorage()._unmanagedContainerModel = nil}

  /// Required. Input configuration of the instances on which predictions are
  /// performed. The schema of any single instance may be specified via the
  /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
  /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
  public var inputConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig {
    get {return _storage._inputConfig ?? Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig()}
    set {_uniqueStorage()._inputConfig = newValue}
  }
  /// Returns true if `inputConfig` has been explicitly set.
  public var hasInputConfig: Bool {return _storage._inputConfig != nil}
  /// Clears the value of `inputConfig`. Subsequent reads from it will return its default value.
  public mutating func clearInputConfig() {_uniqueStorage()._inputConfig = nil}

  /// Configuration for how to convert batch prediction input instances to the
  /// prediction instances that are sent to the Model.
  public var instanceConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig {
    get {return _storage._instanceConfig ?? Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig()}
    set {_uniqueStorage()._instanceConfig = newValue}
  }
  /// Returns true if `instanceConfig` has been explicitly set.
  public var hasInstanceConfig: Bool {return _storage._instanceConfig != nil}
  /// Clears the value of `instanceConfig`. Subsequent reads from it will return its default value.
  public mutating func clearInstanceConfig() {_uniqueStorage()._instanceConfig = nil}

  /// The parameters that govern the predictions. The schema of the parameters
  /// may be specified via the
  /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
  /// [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
  public var modelParameters: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._modelParameters ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._modelParameters = newValue}
  }
  /// Returns true if `modelParameters` has been explicitly set.
  public var hasModelParameters: Bool {return _storage._modelParameters != nil}
  /// Clears the value of `modelParameters`. Subsequent reads from it will return its default value.
  public mutating func clearModelParameters() {_uniqueStorage()._modelParameters = nil}

  /// Required. The Configuration specifying where output predictions should
  /// be written.
  /// The schema of any single prediction may be specified as a concatenation
  /// of [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
  /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
  /// and
  /// [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
  public var outputConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig {
    get {return _storage._outputConfig ?? Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig()}
    set {_uniqueStorage()._outputConfig = newValue}
  }
  /// Returns true if `outputConfig` has been explicitly set.
  public var hasOutputConfig: Bool {return _storage._outputConfig != nil}
  /// Clears the value of `outputConfig`. Subsequent reads from it will return its default value.
  public mutating func clearOutputConfig() {_uniqueStorage()._outputConfig = nil}

  /// The config of resources used by the Model during the batch prediction. If
  /// the Model
  /// [supports][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types]
  /// DEDICATED_RESOURCES this config may be provided (and the job will use these
  /// resources), if the Model doesn't support AUTOMATIC_RESOURCES, this config
  /// must be provided.
  public var dedicatedResources: Google_Cloud_Aiplatform_V1_BatchDedicatedResources {
    get {return _storage._dedicatedResources ?? Google_Cloud_Aiplatform_V1_BatchDedicatedResources()}
    set {_uniqueStorage()._dedicatedResources = newValue}
  }
  /// Returns true if `dedicatedResources` has been explicitly set.
  public var hasDedicatedResources: Bool {return _storage._dedicatedResources != nil}
  /// Clears the value of `dedicatedResources`. Subsequent reads from it will return its default value.
  public mutating func clearDedicatedResources() {_uniqueStorage()._dedicatedResources = nil}

  /// The service account that the DeployedModel's container runs as. If not
  /// specified, a system generated one will be used, which
  /// has minimal permissions and the custom container, if used, may not have
  /// enough permission to access other Google Cloud resources.
  ///
  /// Users deploying the Model must have the `iam.serviceAccounts.actAs`
  /// permission on this service account.
  public var serviceAccount: String {
    get {return _storage._serviceAccount}
    set {_uniqueStorage()._serviceAccount = newValue}
  }

  /// Immutable. Parameters configuring the batch behavior. Currently only
  /// applicable when
  /// [dedicated_resources][google.cloud.aiplatform.v1.BatchPredictionJob.dedicated_resources]
  /// are used (in other cases Vertex AI does the tuning itself).
  public var manualBatchTuningParameters: Google_Cloud_Aiplatform_V1_ManualBatchTuningParameters {
    get {return _storage._manualBatchTuningParameters ?? Google_Cloud_Aiplatform_V1_ManualBatchTuningParameters()}
    set {_uniqueStorage()._manualBatchTuningParameters = newValue}
  }
  /// Returns true if `manualBatchTuningParameters` has been explicitly set.
  public var hasManualBatchTuningParameters: Bool {return _storage._manualBatchTuningParameters != nil}
  /// Clears the value of `manualBatchTuningParameters`. Subsequent reads from it will return its default value.
  public mutating func clearManualBatchTuningParameters() {_uniqueStorage()._manualBatchTuningParameters = nil}

  /// Generate explanation with the batch prediction results.
  ///
  /// When set to `true`, the batch prediction output changes based on the
  /// `predictions_format` field of the
  /// [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config]
  /// object:
  ///
  ///  * `bigquery`: output includes a column named `explanation`. The value
  ///    is a struct that conforms to the
  ///    [Explanation][google.cloud.aiplatform.v1.Explanation] object.
  ///  * `jsonl`: The JSON objects on each line include an additional entry
  ///    keyed `explanation`. The value of the entry is a JSON object that
  ///    conforms to the [Explanation][google.cloud.aiplatform.v1.Explanation]
  ///    object.
  ///  * `csv`: Generating explanations for CSV format is not supported.
  ///
  /// If this field is set to true, either the
  /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
  /// or
  /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
  /// must be populated.
  public var generateExplanation: Bool {
    get {return _storage._generateExplanation}
    set {_uniqueStorage()._generateExplanation = newValue}
  }

  /// Explanation configuration for this BatchPredictionJob. Can be
  /// specified only if
  /// [generate_explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
  /// is set to `true`.
  ///
  /// This value overrides the value of
  /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec].
  /// All fields of
  /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
  /// are optional in the request. If a field of the
  /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
  /// object is not populated, the corresponding field of the
  /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
  /// object is inherited.
  public var explanationSpec: Google_Cloud_Aiplatform_V1_ExplanationSpec {
    get {return _storage._explanationSpec ?? Google_Cloud_Aiplatform_V1_ExplanationSpec()}
    set {_uniqueStorage()._explanationSpec = newValue}
  }
  /// Returns true if `explanationSpec` has been explicitly set.
  public var hasExplanationSpec: Bool {return _storage._explanationSpec != nil}
  /// Clears the value of `explanationSpec`. Subsequent reads from it will return its default value.
  public mutating func clearExplanationSpec() {_uniqueStorage()._explanationSpec = nil}

  /// Output only. Information further describing the output of this job.
  public var outputInfo: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo {
    get {return _storage._outputInfo ?? Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo()}
    set {_uniqueStorage()._outputInfo = newValue}
  }
  /// Returns true if `outputInfo` has been explicitly set.
  public var hasOutputInfo: Bool {return _storage._outputInfo != nil}
  /// Clears the value of `outputInfo`. Subsequent reads from it will return its default value.
  public mutating func clearOutputInfo() {_uniqueStorage()._outputInfo = nil}

  /// Output only. The detailed state of the job.
  public var state: Google_Cloud_Aiplatform_V1_JobState {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. Only populated when the job's state is JOB_STATE_FAILED or
  /// JOB_STATE_CANCELLED.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// Output only. Partial failures encountered.
  /// For example, single files that can't be read.
  /// This field never exceeds 20 entries.
  /// Status details fields contain standard Google Cloud error details.
  public var partialFailures: [Google_Rpc_Status] {
    get {return _storage._partialFailures}
    set {_uniqueStorage()._partialFailures = newValue}
  }

  /// Output only. Information about resources that had been consumed by this
  /// job. Provided in real time at best effort basis, as well as a final value
  /// once the job completes.
  ///
  /// Note: This field currently may be not populated for batch predictions that
  /// use AutoML Models.
  public var resourcesConsumed: Google_Cloud_Aiplatform_V1_ResourcesConsumed {
    get {return _storage._resourcesConsumed ?? Google_Cloud_Aiplatform_V1_ResourcesConsumed()}
    set {_uniqueStorage()._resourcesConsumed = newValue}
  }
  /// Returns true if `resourcesConsumed` has been explicitly set.
  public var hasResourcesConsumed: Bool {return _storage._resourcesConsumed != nil}
  /// Clears the value of `resourcesConsumed`. Subsequent reads from it will return its default value.
  public mutating func clearResourcesConsumed() {_uniqueStorage()._resourcesConsumed = nil}

  /// Output only. Statistics on completed and failed prediction instances.
  public var completionStats: Google_Cloud_Aiplatform_V1_CompletionStats {
    get {return _storage._completionStats ?? Google_Cloud_Aiplatform_V1_CompletionStats()}
    set {_uniqueStorage()._completionStats = newValue}
  }
  /// Returns true if `completionStats` has been explicitly set.
  public var hasCompletionStats: Bool {return _storage._completionStats != nil}
  /// Clears the value of `completionStats`. Subsequent reads from it will return its default value.
  public mutating func clearCompletionStats() {_uniqueStorage()._completionStats = nil}

  /// Output only. Time when the BatchPredictionJob was created.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Time when the BatchPredictionJob for the first time entered
  /// the `JOB_STATE_RUNNING` state.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Time when the BatchPredictionJob entered any of the following
  /// states: `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. Time when the BatchPredictionJob was most recently updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return _storage._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {_uniqueStorage()._updateTime = nil}

  /// The labels with user-defined metadata to organize BatchPredictionJobs.
  ///
  /// Label keys and values can be no longer than 64 characters
  /// (Unicode codepoints), can only contain lowercase letters, numeric
  /// characters, underscores and dashes. International characters are allowed.
  ///
  /// See https://goo.gl/xmQnxf for more information and examples of labels.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  /// Customer-managed encryption key options for a BatchPredictionJob. If this
  /// is set, then all resources created by the BatchPredictionJob will be
  /// encrypted with the provided encryption key.
  public var encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec {
    get {return _storage._encryptionSpec ?? Google_Cloud_Aiplatform_V1_EncryptionSpec()}
    set {_uniqueStorage()._encryptionSpec = newValue}
  }
  /// Returns true if `encryptionSpec` has been explicitly set.
  public var hasEncryptionSpec: Bool {return _storage._encryptionSpec != nil}
  /// Clears the value of `encryptionSpec`. Subsequent reads from it will return its default value.
  public mutating func clearEncryptionSpec() {_uniqueStorage()._encryptionSpec = nil}

  /// For custom-trained Models and AutoML Tabular Models, the container of the
  /// DeployedModel instances will send `stderr` and `stdout` streams to
  /// Cloud Logging by default. Please note that the logs incur cost,
  /// which are subject to [Cloud Logging
  /// pricing](https://cloud.google.com/logging/pricing).
  ///
  /// User can disable container logging by setting this flag to true.
  public var disableContainerLogging: Bool {
    get {return _storage._disableContainerLogging}
    set {_uniqueStorage()._disableContainerLogging = newValue}
  }

  /// Output only. Reserved for future use.
  public var satisfiesPzs: Bool {
    get {return _storage._satisfiesPzs}
    set {_uniqueStorage()._satisfiesPzs = newValue}
  }

  /// Output only. Reserved for future use.
  public var satisfiesPzi: Bool {
    get {return _storage._satisfiesPzi}
    set {_uniqueStorage()._satisfiesPzi = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Configures the input to
  /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
  /// [Model.supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
  /// for Model's supported input formats, and how instances should be expressed
  /// via any of them.
  public struct InputConfig: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Required. The source of the input.
    public var source: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig.OneOf_Source? = nil

    /// The Cloud Storage location for the input instances.
    public var gcsSource: Google_Cloud_Aiplatform_V1_GcsSource {
      get {
        if case .gcsSource(let v)? = source {return v}
        return Google_Cloud_Aiplatform_V1_GcsSource()
      }
      set {source = .gcsSource(newValue)}
    }

    /// The BigQuery location of the input table.
    /// The schema of the table should be in the format described by the given
    /// context OpenAPI Schema, if one is provided. The table may contain
    /// additional columns that are not described by the schema, and they will
    /// be ignored.
    public var bigquerySource: Google_Cloud_Aiplatform_V1_BigQuerySource {
      get {
        if case .bigquerySource(let v)? = source {return v}
        return Google_Cloud_Aiplatform_V1_BigQuerySource()
      }
      set {source = .bigquerySource(newValue)}
    }

    /// Required. The format in which instances are given, must be one of the
    /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
    /// [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats].
    public var instancesFormat: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Required. The source of the input.
    public enum OneOf_Source: Equatable, Sendable {
      /// The Cloud Storage location for the input instances.
      case gcsSource(Google_Cloud_Aiplatform_V1_GcsSource)
      /// The BigQuery location of the input table.
      /// The schema of the table should be in the format described by the given
      /// context OpenAPI Schema, if one is provided. The table may contain
      /// additional columns that are not described by the schema, and they will
      /// be ignored.
      case bigquerySource(Google_Cloud_Aiplatform_V1_BigQuerySource)

    }

    public init() {}
  }

  /// Configuration defining how to transform batch prediction input instances to
  /// the instances that the Model accepts.
  public struct InstanceConfig: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The format of the instance that the Model accepts. Vertex AI will
    /// convert compatible
    /// [batch prediction input instance
    /// formats][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.instances_format]
    /// to the specified format.
    ///
    /// Supported values are:
    ///
    /// * `object`: Each input is converted to JSON object format.
    ///     * For `bigquery`, each row is converted to an object.
    ///     * For `jsonl`, each line of the JSONL input must be an object.
    ///     * Does not apply to `csv`, `file-list`, `tf-record`, or
    ///       `tf-record-gzip`.
    ///
    /// * `array`: Each input is converted to JSON array format.
    ///     * For `bigquery`, each row is converted to an array. The order
    ///       of columns is determined by the BigQuery column order, unless
    ///       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
    ///       is populated.
    ///       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
    ///       must be populated for specifying field orders.
    ///     * For `jsonl`, if each line of the JSONL input is an object,
    ///       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
    ///       must be populated for specifying field orders.
    ///     * Does not apply to `csv`, `file-list`, `tf-record`, or
    ///       `tf-record-gzip`.
    ///
    /// If not specified, Vertex AI converts the batch prediction input as
    /// follows:
    ///
    ///  * For `bigquery` and `csv`, the behavior is the same as `array`. The
    ///    order of columns is the same as defined in the file or table, unless
    ///    [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
    ///    is populated.
    ///  * For `jsonl`, the prediction instance format is determined by
    ///    each line of the input.
    ///  * For `tf-record`/`tf-record-gzip`, each record will be converted to
    ///    an object in the format of `{"b64": <value>}`, where `<value>` is
    ///    the Base64-encoded string of the content of the record.
    ///  * For `file-list`, each file in the list will be converted to an
    ///    object in the format of `{"b64": <value>}`, where `<value>` is
    ///    the Base64-encoded string of the content of the file.
    public var instanceType: String = String()

    /// The name of the field that is considered as a key.
    ///
    /// The values identified by the key field is not included in the transformed
    /// instances that is sent to the Model. This is similar to
    /// specifying this name of the field in
    /// [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields].
    /// In addition, the batch prediction output will not include the instances.
    /// Instead the output will only include the value of the key field, in a
    /// field named `key` in the output:
    ///
    ///  * For `jsonl` output format, the output will have a `key` field
    ///    instead of the `instance` field.
    ///  * For `csv`/`bigquery` output format, the output will have have a `key`
    ///    column instead of the instance feature columns.
    ///
    /// The input must be JSONL with objects at each line, CSV, BigQuery
    /// or TfRecord.
    public var keyField: String = String()

    /// Fields that will be included in the prediction instance that is
    /// sent to the Model.
    ///
    /// If
    /// [instance_type][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.instance_type]
    /// is `array`, the order of field names in included_fields also determines
    /// the order of the values in the array.
    ///
    /// When included_fields is populated,
    /// [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields]
    /// must be empty.
    ///
    /// The input must be JSONL with objects at each line, BigQuery
    /// or TfRecord.
    public var includedFields: [String] = []

    /// Fields that will be excluded in the prediction instance that is
    /// sent to the Model.
    ///
    /// Excluded will be attached to the batch prediction output if
    /// [key_field][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.key_field]
    /// is not specified.
    ///
    /// When excluded_fields is populated,
    /// [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
    /// must be empty.
    ///
    /// The input must be JSONL with objects at each line, BigQuery
    /// or TfRecord.
    public var excludedFields: [String] = []

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  /// Configures the output of
  /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
  /// [Model.supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats]
  /// for supported output formats, and how predictions are expressed via any of
  /// them.
  public struct OutputConfig: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Required. The destination of the output.
    public var destination: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig.OneOf_Destination? = nil

    /// The Cloud Storage location of the directory where the output is
    /// to be written to. In the given directory a new directory is created.
    /// Its name is `prediction-<model-display-name>-<job-create-time>`,
    /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
    /// Inside of it files `predictions_0001.<extension>`,
    /// `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
    /// are created where `<extension>` depends on chosen
    /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format],
    /// and N may equal 0001 and depends on the total number of successfully
    /// predicted instances. If the Model has both
    /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// and
    /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
    /// schemata defined then each such file contains predictions as per the
    /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format].
    /// If prediction for any instance failed (partially or completely), then
    /// an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
    /// `errors_N.<extension>` files are created (N depends on total number
    /// of failed predictions). These files contain the failed instances,
    /// as per their schema, followed by an additional `error` field which as
    /// value has [google.rpc.Status][google.rpc.Status]
    /// containing only `code` and `message` fields.
    public var gcsDestination: Google_Cloud_Aiplatform_V1_GcsDestination {
      get {
        if case .gcsDestination(let v)? = destination {return v}
        return Google_Cloud_Aiplatform_V1_GcsDestination()
      }
      set {destination = .gcsDestination(newValue)}
    }

    /// The BigQuery project or dataset location where the output is to be
    /// written to. If project is provided, a new dataset is created with name
    /// `prediction_<model-display-name>_<job-create-time>`
    /// where <model-display-name> is made
    /// BigQuery-dataset-name compatible (for example, most special characters
    /// become underscores), and timestamp is in
    /// YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
    /// two tables will be created, `predictions`, and `errors`.
    /// If the Model has both
    /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// and
    /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
    /// schemata defined then the tables have columns as follows: The
    /// `predictions` table contains instances for which the prediction
    /// succeeded, it has columns as per a concatenation of the Model's
    /// instance and prediction schemata. The `errors` table contains rows for
    /// which the prediction has failed, it has instance columns, as per the
    /// instance schema, followed by a single "errors" column, which as values
    /// has [google.rpc.Status][google.rpc.Status]
    /// represented as a STRUCT, and containing only `code` and `message`.
    public var bigqueryDestination: Google_Cloud_Aiplatform_V1_BigQueryDestination {
      get {
        if case .bigqueryDestination(let v)? = destination {return v}
        return Google_Cloud_Aiplatform_V1_BigQueryDestination()
      }
      set {destination = .bigqueryDestination(newValue)}
    }

    /// Required. The format in which Vertex AI gives the predictions, must be
    /// one of the [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
    /// [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
    public var predictionsFormat: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Required. The destination of the output.
    public enum OneOf_Destination: Equatable, Sendable {
      /// The Cloud Storage location of the directory where the output is
      /// to be written to. In the given directory a new directory is created.
      /// Its name is `prediction-<model-display-name>-<job-create-time>`,
      /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
      /// Inside of it files `predictions_0001.<extension>`,
      /// `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
      /// are created where `<extension>` depends on chosen
      /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format],
      /// and N may equal 0001 and depends on the total number of successfully
      /// predicted instances. If the Model has both
      /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
      /// and
      /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
      /// schemata defined then each such file contains predictions as per the
      /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format].
      /// If prediction for any instance failed (partially or completely), then
      /// an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
      /// `errors_N.<extension>` files are created (N depends on total number
      /// of failed predictions). These files contain the failed instances,
      /// as per their schema, followed by an additional `error` field which as
      /// value has [google.rpc.Status][google.rpc.Status]
      /// containing only `code` and `message` fields.
      case gcsDestination(Google_Cloud_Aiplatform_V1_GcsDestination)
      /// The BigQuery project or dataset location where the output is to be
      /// written to. If project is provided, a new dataset is created with name
      /// `prediction_<model-display-name>_<job-create-time>`
      /// where <model-display-name> is made
      /// BigQuery-dataset-name compatible (for example, most special characters
      /// become underscores), and timestamp is in
      /// YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
      /// two tables will be created, `predictions`, and `errors`.
      /// If the Model has both
      /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
      /// and
      /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
      /// schemata defined then the tables have columns as follows: The
      /// `predictions` table contains instances for which the prediction
      /// succeeded, it has columns as per a concatenation of the Model's
      /// instance and prediction schemata. The `errors` table contains rows for
      /// which the prediction has failed, it has instance columns, as per the
      /// instance schema, followed by a single "errors" column, which as values
      /// has [google.rpc.Status][google.rpc.Status]
      /// represented as a STRUCT, and containing only `code` and `message`.
      case bigqueryDestination(Google_Cloud_Aiplatform_V1_BigQueryDestination)

    }

    public init() {}
  }

  /// Further describes this job's output.
  /// Supplements
  /// [output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
  public struct OutputInfo: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The output location into which prediction output is written.
    public var outputLocation: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo.OneOf_OutputLocation? = nil

    /// Output only. The full path of the Cloud Storage directory created, into
    /// which the prediction output is written.
    public var gcsOutputDirectory: String {
      get {
        if case .gcsOutputDirectory(let v)? = outputLocation {return v}
        return String()
      }
      set {outputLocation = .gcsOutputDirectory(newValue)}
    }

    /// Output only. The path of the BigQuery dataset created, in
    /// `bq://projectId.bqDatasetId`
    /// format, into which the prediction output is written.
    public var bigqueryOutputDataset: String {
      get {
        if case .bigqueryOutputDataset(let v)? = outputLocation {return v}
        return String()
      }
      set {outputLocation = .bigqueryOutputDataset(newValue)}
    }

    /// Output only. The name of the BigQuery table created, in
    /// `predictions_<timestamp>`
    /// format, into which the prediction output is written.
    /// Can be used by UI to generate the BigQuery output path, for example.
    public var bigqueryOutputTable: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// The output location into which prediction output is written.
    public enum OneOf_OutputLocation: Equatable, Sendable {
      /// Output only. The full path of the Cloud Storage directory created, into
      /// which the prediction output is written.
      case gcsOutputDirectory(String)
      /// Output only. The path of the BigQuery dataset created, in
      /// `bq://projectId.bqDatasetId`
      /// format, into which the prediction output is written.
      case bigqueryOutputDataset(String)

    }

    public init() {}
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1"

extension Google_Cloud_Aiplatform_V1_BatchPredictionJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".BatchPredictionJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .same(proto: "model"),
    30: .standard(proto: "model_version_id"),
    28: .standard(proto: "unmanaged_container_model"),
    4: .standard(proto: "input_config"),
    27: .standard(proto: "instance_config"),
    5: .standard(proto: "model_parameters"),
    6: .standard(proto: "output_config"),
    7: .standard(proto: "dedicated_resources"),
    29: .standard(proto: "service_account"),
    8: .standard(proto: "manual_batch_tuning_parameters"),
    23: .standard(proto: "generate_explanation"),
    25: .standard(proto: "explanation_spec"),
    9: .standard(proto: "output_info"),
    10: .same(proto: "state"),
    11: .same(proto: "error"),
    12: .standard(proto: "partial_failures"),
    13: .standard(proto: "resources_consumed"),
    14: .standard(proto: "completion_stats"),
    15: .standard(proto: "create_time"),
    16: .standard(proto: "start_time"),
    17: .standard(proto: "end_time"),
    18: .standard(proto: "update_time"),
    19: .same(proto: "labels"),
    24: .standard(proto: "encryption_spec"),
    34: .standard(proto: "disable_container_logging"),
    36: .standard(proto: "satisfies_pzs"),
    37: .standard(proto: "satisfies_pzi"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _displayName: String = String()
    var _model: String = String()
    var _modelVersionID: String = String()
    var _unmanagedContainerModel: Google_Cloud_Aiplatform_V1_UnmanagedContainerModel? = nil
    var _inputConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig? = nil
    var _instanceConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig? = nil
    var _modelParameters: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _outputConfig: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig? = nil
    var _dedicatedResources: Google_Cloud_Aiplatform_V1_BatchDedicatedResources? = nil
    var _serviceAccount: String = String()
    var _manualBatchTuningParameters: Google_Cloud_Aiplatform_V1_ManualBatchTuningParameters? = nil
    var _generateExplanation: Bool = false
    var _explanationSpec: Google_Cloud_Aiplatform_V1_ExplanationSpec? = nil
    var _outputInfo: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo? = nil
    var _state: Google_Cloud_Aiplatform_V1_JobState = .unspecified
    var _error: Google_Rpc_Status? = nil
    var _partialFailures: [Google_Rpc_Status] = []
    var _resourcesConsumed: Google_Cloud_Aiplatform_V1_ResourcesConsumed? = nil
    var _completionStats: Google_Cloud_Aiplatform_V1_CompletionStats? = nil
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _labels: Dictionary<String,String> = [:]
    var _encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec? = nil
    var _disableContainerLogging: Bool = false
    var _satisfiesPzs: Bool = false
    var _satisfiesPzi: Bool = false

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _displayName = source._displayName
      _model = source._model
      _modelVersionID = source._modelVersionID
      _unmanagedContainerModel = source._unmanagedContainerModel
      _inputConfig = source._inputConfig
      _instanceConfig = source._instanceConfig
      _modelParameters = source._modelParameters
      _outputConfig = source._outputConfig
      _dedicatedResources = source._dedicatedResources
      _serviceAccount = source._serviceAccount
      _manualBatchTuningParameters = source._manualBatchTuningParameters
      _generateExplanation = source._generateExplanation
      _explanationSpec = source._explanationSpec
      _outputInfo = source._outputInfo
      _state = source._state
      _error = source._error
      _partialFailures = source._partialFailures
      _resourcesConsumed = source._resourcesConsumed
      _completionStats = source._completionStats
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _updateTime = source._updateTime
      _labels = source._labels
      _encryptionSpec = source._encryptionSpec
      _disableContainerLogging = source._disableContainerLogging
      _satisfiesPzs = source._satisfiesPzs
      _satisfiesPzi = source._satisfiesPzi
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._displayName) }()
        case 3: try { try decoder.decodeSingularStringField(value: &_storage._model) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._inputConfig) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._modelParameters) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._outputConfig) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._dedicatedResources) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._manualBatchTuningParameters) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._outputInfo) }()
        case 10: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 12: try { try decoder.decodeRepeatedMessageField(value: &_storage._partialFailures) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._resourcesConsumed) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._completionStats) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 17: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 18: try { try decoder.decodeSingularMessageField(value: &_storage._updateTime) }()
        case 19: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 23: try { try decoder.decodeSingularBoolField(value: &_storage._generateExplanation) }()
        case 24: try { try decoder.decodeSingularMessageField(value: &_storage._encryptionSpec) }()
        case 25: try { try decoder.decodeSingularMessageField(value: &_storage._explanationSpec) }()
        case 27: try { try decoder.decodeSingularMessageField(value: &_storage._instanceConfig) }()
        case 28: try { try decoder.decodeSingularMessageField(value: &_storage._unmanagedContainerModel) }()
        case 29: try { try decoder.decodeSingularStringField(value: &_storage._serviceAccount) }()
        case 30: try { try decoder.decodeSingularStringField(value: &_storage._modelVersionID) }()
        case 34: try { try decoder.decodeSingularBoolField(value: &_storage._disableContainerLogging) }()
        case 36: try { try decoder.decodeSingularBoolField(value: &_storage._satisfiesPzs) }()
        case 37: try { try decoder.decodeSingularBoolField(value: &_storage._satisfiesPzi) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if !_storage._displayName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._displayName, fieldNumber: 2)
      }
      if !_storage._model.isEmpty {
        try visitor.visitSingularStringField(value: _storage._model, fieldNumber: 3)
      }
      try { if let v = _storage._inputConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      try { if let v = _storage._modelParameters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._outputConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      try { if let v = _storage._dedicatedResources {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      try { if let v = _storage._manualBatchTuningParameters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      } }()
      try { if let v = _storage._outputInfo {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 10)
      }
      try { if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      if !_storage._partialFailures.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._partialFailures, fieldNumber: 12)
      }
      try { if let v = _storage._resourcesConsumed {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      } }()
      try { if let v = _storage._completionStats {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      } }()
      try { if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      } }()
      try { if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      } }()
      try { if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 17)
      } }()
      try { if let v = _storage._updateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 18)
      } }()
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 19)
      }
      if _storage._generateExplanation != false {
        try visitor.visitSingularBoolField(value: _storage._generateExplanation, fieldNumber: 23)
      }
      try { if let v = _storage._encryptionSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 24)
      } }()
      try { if let v = _storage._explanationSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 25)
      } }()
      try { if let v = _storage._instanceConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 27)
      } }()
      try { if let v = _storage._unmanagedContainerModel {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 28)
      } }()
      if !_storage._serviceAccount.isEmpty {
        try visitor.visitSingularStringField(value: _storage._serviceAccount, fieldNumber: 29)
      }
      if !_storage._modelVersionID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._modelVersionID, fieldNumber: 30)
      }
      if _storage._disableContainerLogging != false {
        try visitor.visitSingularBoolField(value: _storage._disableContainerLogging, fieldNumber: 34)
      }
      if _storage._satisfiesPzs != false {
        try visitor.visitSingularBoolField(value: _storage._satisfiesPzs, fieldNumber: 36)
      }
      if _storage._satisfiesPzi != false {
        try visitor.visitSingularBoolField(value: _storage._satisfiesPzi, fieldNumber: 37)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob, rhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._displayName != rhs_storage._displayName {return false}
        if _storage._model != rhs_storage._model {return false}
        if _storage._modelVersionID != rhs_storage._modelVersionID {return false}
        if _storage._unmanagedContainerModel != rhs_storage._unmanagedContainerModel {return false}
        if _storage._inputConfig != rhs_storage._inputConfig {return false}
        if _storage._instanceConfig != rhs_storage._instanceConfig {return false}
        if _storage._modelParameters != rhs_storage._modelParameters {return false}
        if _storage._outputConfig != rhs_storage._outputConfig {return false}
        if _storage._dedicatedResources != rhs_storage._dedicatedResources {return false}
        if _storage._serviceAccount != rhs_storage._serviceAccount {return false}
        if _storage._manualBatchTuningParameters != rhs_storage._manualBatchTuningParameters {return false}
        if _storage._generateExplanation != rhs_storage._generateExplanation {return false}
        if _storage._explanationSpec != rhs_storage._explanationSpec {return false}
        if _storage._outputInfo != rhs_storage._outputInfo {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._partialFailures != rhs_storage._partialFailures {return false}
        if _storage._resourcesConsumed != rhs_storage._resourcesConsumed {return false}
        if _storage._completionStats != rhs_storage._completionStats {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._updateTime != rhs_storage._updateTime {return false}
        if _storage._labels != rhs_storage._labels {return false}
        if _storage._encryptionSpec != rhs_storage._encryptionSpec {return false}
        if _storage._disableContainerLogging != rhs_storage._disableContainerLogging {return false}
        if _storage._satisfiesPzs != rhs_storage._satisfiesPzs {return false}
        if _storage._satisfiesPzi != rhs_storage._satisfiesPzi {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_BatchPredictionJob.protoMessageName + ".InputConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "gcs_source"),
    3: .standard(proto: "bigquery_source"),
    1: .standard(proto: "instances_format"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.instancesFormat) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1_GcsSource?
        var hadOneofValue = false
        if let current = self.source {
          hadOneofValue = true
          if case .gcsSource(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.source = .gcsSource(v)
        }
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1_BigQuerySource?
        var hadOneofValue = false
        if let current = self.source {
          hadOneofValue = true
          if case .bigquerySource(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.source = .bigquerySource(v)
        }
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.instancesFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.instancesFormat, fieldNumber: 1)
    }
    switch self.source {
    case .gcsSource?: try {
      guard case .gcsSource(let v)? = self.source else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .bigquerySource?: try {
      guard case .bigquerySource(let v)? = self.source else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig, rhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InputConfig) -> Bool {
    if lhs.source != rhs.source {return false}
    if lhs.instancesFormat != rhs.instancesFormat {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_BatchPredictionJob.protoMessageName + ".InstanceConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "instance_type"),
    2: .standard(proto: "key_field"),
    3: .standard(proto: "included_fields"),
    4: .standard(proto: "excluded_fields"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.instanceType) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.keyField) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.includedFields) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.excludedFields) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.instanceType.isEmpty {
      try visitor.visitSingularStringField(value: self.instanceType, fieldNumber: 1)
    }
    if !self.keyField.isEmpty {
      try visitor.visitSingularStringField(value: self.keyField, fieldNumber: 2)
    }
    if !self.includedFields.isEmpty {
      try visitor.visitRepeatedStringField(value: self.includedFields, fieldNumber: 3)
    }
    if !self.excludedFields.isEmpty {
      try visitor.visitRepeatedStringField(value: self.excludedFields, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig, rhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.InstanceConfig) -> Bool {
    if lhs.instanceType != rhs.instanceType {return false}
    if lhs.keyField != rhs.keyField {return false}
    if lhs.includedFields != rhs.includedFields {return false}
    if lhs.excludedFields != rhs.excludedFields {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_BatchPredictionJob.protoMessageName + ".OutputConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "gcs_destination"),
    3: .standard(proto: "bigquery_destination"),
    1: .standard(proto: "predictions_format"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.predictionsFormat) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1_GcsDestination?
        var hadOneofValue = false
        if let current = self.destination {
          hadOneofValue = true
          if case .gcsDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.destination = .gcsDestination(v)
        }
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1_BigQueryDestination?
        var hadOneofValue = false
        if let current = self.destination {
          hadOneofValue = true
          if case .bigqueryDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.destination = .bigqueryDestination(v)
        }
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.predictionsFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.predictionsFormat, fieldNumber: 1)
    }
    switch self.destination {
    case .gcsDestination?: try {
      guard case .gcsDestination(let v)? = self.destination else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .bigqueryDestination?: try {
      guard case .bigqueryDestination(let v)? = self.destination else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig, rhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputConfig) -> Bool {
    if lhs.destination != rhs.destination {return false}
    if lhs.predictionsFormat != rhs.predictionsFormat {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_BatchPredictionJob.protoMessageName + ".OutputInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "gcs_output_directory"),
    2: .standard(proto: "bigquery_output_dataset"),
    4: .standard(proto: "bigquery_output_table"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {
          if self.outputLocation != nil {try decoder.handleConflictingOneOf()}
          self.outputLocation = .gcsOutputDirectory(v)
        }
      }()
      case 2: try {
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {
          if self.outputLocation != nil {try decoder.handleConflictingOneOf()}
          self.outputLocation = .bigqueryOutputDataset(v)
        }
      }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.bigqueryOutputTable) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    switch self.outputLocation {
    case .gcsOutputDirectory?: try {
      guard case .gcsOutputDirectory(let v)? = self.outputLocation else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .bigqueryOutputDataset?: try {
      guard case .bigqueryOutputDataset(let v)? = self.outputLocation else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if !self.bigqueryOutputTable.isEmpty {
      try visitor.visitSingularStringField(value: self.bigqueryOutputTable, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo, rhs: Google_Cloud_Aiplatform_V1_BatchPredictionJob.OutputInfo) -> Bool {
    if lhs.outputLocation != rhs.outputLocation {return false}
    if lhs.bigqueryOutputTable != rhs.bigqueryOutputTable {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
