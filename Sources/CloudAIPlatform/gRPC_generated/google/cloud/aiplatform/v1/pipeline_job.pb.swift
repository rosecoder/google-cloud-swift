// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1/pipeline_job.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// An instance of a machine learning PipelineJob.
public struct Google_Cloud_Aiplatform_V1_PipelineJob: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The resource name of the PipelineJob.
  public var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// The display name of the Pipeline.
  /// The name can be up to 128 characters long and can consist of any UTF-8
  /// characters.
  public var displayName: String {
    get {return _storage._displayName}
    set {_uniqueStorage()._displayName = newValue}
  }

  /// Output only. Pipeline creation time.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Pipeline start time.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Pipeline end time.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. Timestamp when this PipelineJob was most recently updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return _storage._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {_uniqueStorage()._updateTime = nil}

  /// The spec of the pipeline.
  public var pipelineSpec: SwiftProtobuf.Google_Protobuf_Struct {
    get {return _storage._pipelineSpec ?? SwiftProtobuf.Google_Protobuf_Struct()}
    set {_uniqueStorage()._pipelineSpec = newValue}
  }
  /// Returns true if `pipelineSpec` has been explicitly set.
  public var hasPipelineSpec: Bool {return _storage._pipelineSpec != nil}
  /// Clears the value of `pipelineSpec`. Subsequent reads from it will return its default value.
  public mutating func clearPipelineSpec() {_uniqueStorage()._pipelineSpec = nil}

  /// Output only. The detailed state of the job.
  public var state: Google_Cloud_Aiplatform_V1_PipelineState {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. The details of pipeline run. Not available in the list view.
  public var jobDetail: Google_Cloud_Aiplatform_V1_PipelineJobDetail {
    get {return _storage._jobDetail ?? Google_Cloud_Aiplatform_V1_PipelineJobDetail()}
    set {_uniqueStorage()._jobDetail = newValue}
  }
  /// Returns true if `jobDetail` has been explicitly set.
  public var hasJobDetail: Bool {return _storage._jobDetail != nil}
  /// Clears the value of `jobDetail`. Subsequent reads from it will return its default value.
  public mutating func clearJobDetail() {_uniqueStorage()._jobDetail = nil}

  /// Output only. The error that occurred during pipeline execution.
  /// Only populated when the pipeline's state is FAILED or CANCELLED.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// The labels with user-defined metadata to organize PipelineJob.
  ///
  /// Label keys and values can be no longer than 64 characters
  /// (Unicode codepoints), can only contain lowercase letters, numeric
  /// characters, underscores and dashes. International characters are allowed.
  ///
  /// See https://goo.gl/xmQnxf for more information and examples of labels.
  ///
  /// Note there is some reserved label key for Vertex AI Pipelines.
  /// - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  /// Runtime config of the pipeline.
  public var runtimeConfig: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig {
    get {return _storage._runtimeConfig ?? Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig()}
    set {_uniqueStorage()._runtimeConfig = newValue}
  }
  /// Returns true if `runtimeConfig` has been explicitly set.
  public var hasRuntimeConfig: Bool {return _storage._runtimeConfig != nil}
  /// Clears the value of `runtimeConfig`. Subsequent reads from it will return its default value.
  public mutating func clearRuntimeConfig() {_uniqueStorage()._runtimeConfig = nil}

  /// Customer-managed encryption key spec for a pipelineJob. If set, this
  /// PipelineJob and all of its sub-resources will be secured by this key.
  public var encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec {
    get {return _storage._encryptionSpec ?? Google_Cloud_Aiplatform_V1_EncryptionSpec()}
    set {_uniqueStorage()._encryptionSpec = newValue}
  }
  /// Returns true if `encryptionSpec` has been explicitly set.
  public var hasEncryptionSpec: Bool {return _storage._encryptionSpec != nil}
  /// Clears the value of `encryptionSpec`. Subsequent reads from it will return its default value.
  public mutating func clearEncryptionSpec() {_uniqueStorage()._encryptionSpec = nil}

  /// The service account that the pipeline workload runs as.
  /// If not specified, the Compute Engine default service account in the project
  /// will be used.
  /// See
  /// https://cloud.google.com/compute/docs/access/service-accounts#default_service_account
  ///
  /// Users starting the pipeline must have the `iam.serviceAccounts.actAs`
  /// permission on this service account.
  public var serviceAccount: String {
    get {return _storage._serviceAccount}
    set {_uniqueStorage()._serviceAccount = newValue}
  }

  /// The full name of the Compute Engine
  /// [network](/compute/docs/networks-and-firewalls#networks) to which the
  /// Pipeline Job's workload should be peered. For example,
  /// `projects/12345/global/networks/myVPC`.
  /// [Format](/compute/docs/reference/rest/v1/networks/insert)
  /// is of the form `projects/{project}/global/networks/{network}`.
  /// Where {project} is a project number, as in `12345`, and {network} is a
  /// network name.
  ///
  /// Private services access must already be configured for the network.
  /// Pipeline job will apply the network configuration to the Google Cloud
  /// resources being launched, if applied, such as Vertex AI
  /// Training or Dataflow job. If left unspecified, the workload is not peered
  /// with any network.
  public var network: String {
    get {return _storage._network}
    set {_uniqueStorage()._network = newValue}
  }

  /// A list of names for the reserved ip ranges under the VPC network
  /// that can be used for this Pipeline Job's workload.
  ///
  /// If set, we will deploy the Pipeline Job's workload within the provided ip
  /// ranges. Otherwise, the job will be deployed to any ip ranges under the
  /// provided VPC network.
  ///
  /// Example: ['vertex-ai-ip-range'].
  public var reservedIpRanges: [String] {
    get {return _storage._reservedIpRanges}
    set {_uniqueStorage()._reservedIpRanges = newValue}
  }

  /// A template uri from where the
  /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec],
  /// if empty, will be downloaded. Currently, only uri from Vertex Template
  /// Registry & Gallery is supported. Reference to
  /// https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
  public var templateUri: String {
    get {return _storage._templateUri}
    set {_uniqueStorage()._templateUri = newValue}
  }

  /// Output only. Pipeline template metadata. Will fill up fields if
  /// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
  /// is from supported template registry.
  public var templateMetadata: Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata {
    get {return _storage._templateMetadata ?? Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata()}
    set {_uniqueStorage()._templateMetadata = newValue}
  }
  /// Returns true if `templateMetadata` has been explicitly set.
  public var hasTemplateMetadata: Bool {return _storage._templateMetadata != nil}
  /// Clears the value of `templateMetadata`. Subsequent reads from it will return its default value.
  public mutating func clearTemplateMetadata() {_uniqueStorage()._templateMetadata = nil}

  /// Output only. The schedule resource name.
  /// Only returned if the Pipeline is created by Schedule API.
  public var scheduleName: String {
    get {return _storage._scheduleName}
    set {_uniqueStorage()._scheduleName = newValue}
  }

  /// Optional. Whether to do component level validations before job creation.
  public var preflightValidations: Bool {
    get {return _storage._preflightValidations}
    set {_uniqueStorage()._preflightValidations = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The runtime config of a PipelineJob.
  public struct RuntimeConfig: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Deprecated. Use
    /// [RuntimeConfig.parameter_values][google.cloud.aiplatform.v1.PipelineJob.RuntimeConfig.parameter_values]
    /// instead. The runtime parameters of the PipelineJob. The parameters will
    /// be passed into
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
    /// to replace the placeholders at runtime. This field is used by pipelines
    /// built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower,
    /// such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
    ///
    /// NOTE: This field was marked as deprecated in the .proto file.
    public var parameters: Dictionary<String,Google_Cloud_Aiplatform_V1_Value> = [:]

    /// Required. A path in a Cloud Storage bucket, which will be treated as the
    /// root output directory of the pipeline. It is used by the system to
    /// generate the paths of output artifacts. The artifact paths are generated
    /// with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the
    /// specified output directory. The service account specified in this
    /// pipeline must have the `storage.objects.get` and `storage.objects.create`
    /// permissions for this bucket.
    public var gcsOutputDirectory: String = String()

    /// The runtime parameters of the PipelineJob. The parameters will be
    /// passed into
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
    /// to replace the placeholders at runtime. This field is used by pipelines
    /// built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as
    /// pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2
    /// DSL.
    public var parameterValues: Dictionary<String,SwiftProtobuf.Google_Protobuf_Value> = [:]

    /// Represents the failure policy of a pipeline. Currently, the default of a
    /// pipeline is that the pipeline will continue to run until no more tasks
    /// can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW.
    /// However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it
    /// will stop scheduling any new tasks when a task has failed. Any scheduled
    /// tasks will continue to completion.
    public var failurePolicy: Google_Cloud_Aiplatform_V1_PipelineFailurePolicy = .unspecified

    /// The runtime artifacts of the PipelineJob. The key will be the input
    /// artifact name and the value would be one of the InputArtifact.
    public var inputArtifacts: Dictionary<String,Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact> = [:]

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// The type of an input artifact.
    public struct InputArtifact: Sendable {
      // SwiftProtobuf.Message conformance is added in an extension below. See the
      // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
      // methods supported on all messages.

      public var kind: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact.OneOf_Kind? = nil

      /// Artifact resource id from MLMD. Which is the last portion of an
      /// artifact resource name:
      /// `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`.
      /// The artifact must stay within the same project, location and default
      /// metadatastore as the pipeline.
      public var artifactID: String {
        get {
          if case .artifactID(let v)? = kind {return v}
          return String()
        }
        set {kind = .artifactID(newValue)}
      }

      public var unknownFields = SwiftProtobuf.UnknownStorage()

      public enum OneOf_Kind: Equatable, Sendable {
        /// Artifact resource id from MLMD. Which is the last portion of an
        /// artifact resource name:
        /// `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`.
        /// The artifact must stay within the same project, location and default
        /// metadatastore as the pipeline.
        case artifactID(String)

      }

      public init() {}
    }

    public init() {}
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Pipeline template metadata if
/// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
/// is from supported template registry. Currently, the only supported registry
/// is Artifact Registry.
public struct Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The version_name in artifact registry.
  ///
  /// Will always be presented in output if the
  /// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
  /// is from supported template registry.
  ///
  /// Format is "sha256:abcdef123456...".
  public var version: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The runtime detail of PipelineJob.
public struct Google_Cloud_Aiplatform_V1_PipelineJobDetail: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The context of the pipeline.
  public var pipelineContext: Google_Cloud_Aiplatform_V1_Context {
    get {return _storage._pipelineContext ?? Google_Cloud_Aiplatform_V1_Context()}
    set {_uniqueStorage()._pipelineContext = newValue}
  }
  /// Returns true if `pipelineContext` has been explicitly set.
  public var hasPipelineContext: Bool {return _storage._pipelineContext != nil}
  /// Clears the value of `pipelineContext`. Subsequent reads from it will return its default value.
  public mutating func clearPipelineContext() {_uniqueStorage()._pipelineContext = nil}

  /// Output only. The context of the current pipeline run.
  public var pipelineRunContext: Google_Cloud_Aiplatform_V1_Context {
    get {return _storage._pipelineRunContext ?? Google_Cloud_Aiplatform_V1_Context()}
    set {_uniqueStorage()._pipelineRunContext = newValue}
  }
  /// Returns true if `pipelineRunContext` has been explicitly set.
  public var hasPipelineRunContext: Bool {return _storage._pipelineRunContext != nil}
  /// Clears the value of `pipelineRunContext`. Subsequent reads from it will return its default value.
  public mutating func clearPipelineRunContext() {_uniqueStorage()._pipelineRunContext = nil}

  /// Output only. The runtime details of the tasks under the pipeline.
  public var taskDetails: [Google_Cloud_Aiplatform_V1_PipelineTaskDetail] {
    get {return _storage._taskDetails}
    set {_uniqueStorage()._taskDetails = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The runtime detail of a task execution.
public struct Google_Cloud_Aiplatform_V1_PipelineTaskDetail: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The system generated ID of the task.
  public var taskID: Int64 {
    get {return _storage._taskID}
    set {_uniqueStorage()._taskID = newValue}
  }

  /// Output only. The id of the parent task if the task is within a component
  /// scope. Empty if the task is at the root level.
  public var parentTaskID: Int64 {
    get {return _storage._parentTaskID}
    set {_uniqueStorage()._parentTaskID = newValue}
  }

  /// Output only. The user specified name of the task that is defined in
  /// [pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
  public var taskName: String {
    get {return _storage._taskName}
    set {_uniqueStorage()._taskName = newValue}
  }

  /// Output only. Task create time.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Task start time.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Task end time.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. The detailed execution info.
  public var executorDetail: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail {
    get {return _storage._executorDetail ?? Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail()}
    set {_uniqueStorage()._executorDetail = newValue}
  }
  /// Returns true if `executorDetail` has been explicitly set.
  public var hasExecutorDetail: Bool {return _storage._executorDetail != nil}
  /// Clears the value of `executorDetail`. Subsequent reads from it will return its default value.
  public mutating func clearExecutorDetail() {_uniqueStorage()._executorDetail = nil}

  /// Output only. State of the task.
  public var state: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.State {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. The execution metadata of the task.
  public var execution: Google_Cloud_Aiplatform_V1_Execution {
    get {return _storage._execution ?? Google_Cloud_Aiplatform_V1_Execution()}
    set {_uniqueStorage()._execution = newValue}
  }
  /// Returns true if `execution` has been explicitly set.
  public var hasExecution: Bool {return _storage._execution != nil}
  /// Clears the value of `execution`. Subsequent reads from it will return its default value.
  public mutating func clearExecution() {_uniqueStorage()._execution = nil}

  /// Output only. The error that occurred during task execution.
  /// Only populated when the task's state is FAILED or CANCELLED.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// Output only. A list of task status. This field keeps a record of task
  /// status evolving over time.
  public var pipelineTaskStatus: [Google_Cloud_Aiplatform_V1_PipelineTaskDetail.PipelineTaskStatus] {
    get {return _storage._pipelineTaskStatus}
    set {_uniqueStorage()._pipelineTaskStatus = newValue}
  }

  /// Output only. The runtime input artifacts of the task.
  public var inputs: Dictionary<String,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList> {
    get {return _storage._inputs}
    set {_uniqueStorage()._inputs = newValue}
  }

  /// Output only. The runtime output artifacts of the task.
  public var outputs: Dictionary<String,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList> {
    get {return _storage._outputs}
    set {_uniqueStorage()._outputs = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Specifies state of TaskExecution
  public enum State: SwiftProtobuf.Enum, Swift.CaseIterable {
    public typealias RawValue = Int

    /// Unspecified.
    case unspecified // = 0

    /// Specifies pending state for the task.
    case pending // = 1

    /// Specifies task is being executed.
    case running // = 2

    /// Specifies task completed successfully.
    case succeeded // = 3

    /// Specifies Task cancel is in pending state.
    case cancelPending // = 4

    /// Specifies task is being cancelled.
    case cancelling // = 5

    /// Specifies task was cancelled.
    case cancelled // = 6

    /// Specifies task failed.
    case failed // = 7

    /// Specifies task was skipped due to cache hit.
    case skipped // = 8

    /// Specifies that the task was not triggered because the task's trigger
    /// policy is not satisfied. The trigger policy is specified in the
    /// `condition` field of
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
    case notTriggered // = 9
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .pending
      case 2: self = .running
      case 3: self = .succeeded
      case 4: self = .cancelPending
      case 5: self = .cancelling
      case 6: self = .cancelled
      case 7: self = .failed
      case 8: self = .skipped
      case 9: self = .notTriggered
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .pending: return 1
      case .running: return 2
      case .succeeded: return 3
      case .cancelPending: return 4
      case .cancelling: return 5
      case .cancelled: return 6
      case .failed: return 7
      case .skipped: return 8
      case .notTriggered: return 9
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    public static let allCases: [Google_Cloud_Aiplatform_V1_PipelineTaskDetail.State] = [
      .unspecified,
      .pending,
      .running,
      .succeeded,
      .cancelPending,
      .cancelling,
      .cancelled,
      .failed,
      .skipped,
      .notTriggered,
    ]

  }

  /// A single record of the task status.
  public struct PipelineTaskStatus: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. Update time of this status.
    public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
      get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
      set {_updateTime = newValue}
    }
    /// Returns true if `updateTime` has been explicitly set.
    public var hasUpdateTime: Bool {return self._updateTime != nil}
    /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
    public mutating func clearUpdateTime() {self._updateTime = nil}

    /// Output only. The state of the task.
    public var state: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.State = .unspecified

    /// Output only. The error that occurred during the state. May be set when
    /// the state is any of the non-final state (PENDING/RUNNING/CANCELLING) or
    /// FAILED state. If the state is FAILED, the error here is final and not
    /// going to be retried. If the state is a non-final state, the error
    /// indicates a system-error being retried.
    public var error: Google_Rpc_Status {
      get {return _error ?? Google_Rpc_Status()}
      set {_error = newValue}
    }
    /// Returns true if `error` has been explicitly set.
    public var hasError: Bool {return self._error != nil}
    /// Clears the value of `error`. Subsequent reads from it will return its default value.
    public mutating func clearError() {self._error = nil}

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}

    fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    fileprivate var _error: Google_Rpc_Status? = nil
  }

  /// A list of artifact metadata.
  public struct ArtifactList: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. A list of artifact metadata.
    public var artifacts: [Google_Cloud_Aiplatform_V1_Artifact] = []

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The runtime detail of a pipeline executor.
public struct Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var details: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.OneOf_Details? = nil

  /// Output only. The detailed info for a container executor.
  public var containerDetail: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail {
    get {
      if case .containerDetail(let v)? = details {return v}
      return Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail()
    }
    set {details = .containerDetail(newValue)}
  }

  /// Output only. The detailed info for a custom job executor.
  public var customJobDetail: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail {
    get {
      if case .customJobDetail(let v)? = details {return v}
      return Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail()
    }
    set {details = .customJobDetail(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public enum OneOf_Details: Equatable, Sendable {
    /// Output only. The detailed info for a container executor.
    case containerDetail(Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail)
    /// Output only. The detailed info for a custom job executor.
    case customJobDetail(Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail)

  }

  /// The detail of a container execution. It contains the job names of the
  /// lifecycle of a container execution.
  public struct ContainerDetail: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. The name of the
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
    /// execution.
    public var mainJob: String = String()

    /// Output only. The name of the
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
    /// pre-caching-check container execution. This job will be available if the
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
    /// specifies the `pre_caching_check` hook in the lifecycle events.
    public var preCachingCheckJob: String = String()

    /// Output only. The names of the previously failed
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
    /// executions. The list includes the all attempts in chronological order.
    public var failedMainJobs: [String] = []

    /// Output only. The names of the previously failed
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
    /// pre-caching-check container executions. This job will be available if the
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
    /// specifies the `pre_caching_check` hook in the lifecycle events. The list
    /// includes the all attempts in chronological order.
    public var failedPreCachingCheckJobs: [String] = []

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  /// The detailed info for a custom job executor.
  public struct CustomJobDetail: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. The name of the
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob].
    public var job: String = String()

    /// Output only. The names of the previously failed
    /// [CustomJob][google.cloud.aiplatform.v1.CustomJob]. The list includes the
    /// all attempts in chronological order.
    public var failedJobs: [String] = []

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1"

extension Google_Cloud_Aiplatform_V1_PipelineJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PipelineJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .standard(proto: "create_time"),
    4: .standard(proto: "start_time"),
    5: .standard(proto: "end_time"),
    6: .standard(proto: "update_time"),
    7: .standard(proto: "pipeline_spec"),
    8: .same(proto: "state"),
    9: .standard(proto: "job_detail"),
    10: .same(proto: "error"),
    11: .same(proto: "labels"),
    12: .standard(proto: "runtime_config"),
    16: .standard(proto: "encryption_spec"),
    17: .standard(proto: "service_account"),
    18: .same(proto: "network"),
    25: .standard(proto: "reserved_ip_ranges"),
    19: .standard(proto: "template_uri"),
    20: .standard(proto: "template_metadata"),
    22: .standard(proto: "schedule_name"),
    26: .standard(proto: "preflight_validations"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _displayName: String = String()
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _pipelineSpec: SwiftProtobuf.Google_Protobuf_Struct? = nil
    var _state: Google_Cloud_Aiplatform_V1_PipelineState = .unspecified
    var _jobDetail: Google_Cloud_Aiplatform_V1_PipelineJobDetail? = nil
    var _error: Google_Rpc_Status? = nil
    var _labels: Dictionary<String,String> = [:]
    var _runtimeConfig: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig? = nil
    var _encryptionSpec: Google_Cloud_Aiplatform_V1_EncryptionSpec? = nil
    var _serviceAccount: String = String()
    var _network: String = String()
    var _reservedIpRanges: [String] = []
    var _templateUri: String = String()
    var _templateMetadata: Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata? = nil
    var _scheduleName: String = String()
    var _preflightValidations: Bool = false

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _displayName = source._displayName
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _updateTime = source._updateTime
      _pipelineSpec = source._pipelineSpec
      _state = source._state
      _jobDetail = source._jobDetail
      _error = source._error
      _labels = source._labels
      _runtimeConfig = source._runtimeConfig
      _encryptionSpec = source._encryptionSpec
      _serviceAccount = source._serviceAccount
      _network = source._network
      _reservedIpRanges = source._reservedIpRanges
      _templateUri = source._templateUri
      _templateMetadata = source._templateMetadata
      _scheduleName = source._scheduleName
      _preflightValidations = source._preflightValidations
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._displayName) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._updateTime) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._pipelineSpec) }()
        case 8: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._jobDetail) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 11: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._runtimeConfig) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._encryptionSpec) }()
        case 17: try { try decoder.decodeSingularStringField(value: &_storage._serviceAccount) }()
        case 18: try { try decoder.decodeSingularStringField(value: &_storage._network) }()
        case 19: try { try decoder.decodeSingularStringField(value: &_storage._templateUri) }()
        case 20: try { try decoder.decodeSingularMessageField(value: &_storage._templateMetadata) }()
        case 22: try { try decoder.decodeSingularStringField(value: &_storage._scheduleName) }()
        case 25: try { try decoder.decodeRepeatedStringField(value: &_storage._reservedIpRanges) }()
        case 26: try { try decoder.decodeSingularBoolField(value: &_storage._preflightValidations) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if !_storage._displayName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._displayName, fieldNumber: 2)
      }
      try { if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      try { if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      try { if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._updateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      try { if let v = _storage._pipelineSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 8)
      }
      try { if let v = _storage._jobDetail {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
      try { if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 11)
      }
      try { if let v = _storage._runtimeConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      } }()
      try { if let v = _storage._encryptionSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      } }()
      if !_storage._serviceAccount.isEmpty {
        try visitor.visitSingularStringField(value: _storage._serviceAccount, fieldNumber: 17)
      }
      if !_storage._network.isEmpty {
        try visitor.visitSingularStringField(value: _storage._network, fieldNumber: 18)
      }
      if !_storage._templateUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._templateUri, fieldNumber: 19)
      }
      try { if let v = _storage._templateMetadata {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 20)
      } }()
      if !_storage._scheduleName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._scheduleName, fieldNumber: 22)
      }
      if !_storage._reservedIpRanges.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._reservedIpRanges, fieldNumber: 25)
      }
      if _storage._preflightValidations != false {
        try visitor.visitSingularBoolField(value: _storage._preflightValidations, fieldNumber: 26)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineJob, rhs: Google_Cloud_Aiplatform_V1_PipelineJob) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._displayName != rhs_storage._displayName {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._updateTime != rhs_storage._updateTime {return false}
        if _storage._pipelineSpec != rhs_storage._pipelineSpec {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._jobDetail != rhs_storage._jobDetail {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._labels != rhs_storage._labels {return false}
        if _storage._runtimeConfig != rhs_storage._runtimeConfig {return false}
        if _storage._encryptionSpec != rhs_storage._encryptionSpec {return false}
        if _storage._serviceAccount != rhs_storage._serviceAccount {return false}
        if _storage._network != rhs_storage._network {return false}
        if _storage._reservedIpRanges != rhs_storage._reservedIpRanges {return false}
        if _storage._templateUri != rhs_storage._templateUri {return false}
        if _storage._templateMetadata != rhs_storage._templateMetadata {return false}
        if _storage._scheduleName != rhs_storage._scheduleName {return false}
        if _storage._preflightValidations != rhs_storage._preflightValidations {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineJob.protoMessageName + ".RuntimeConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parameters"),
    2: .standard(proto: "gcs_output_directory"),
    3: .standard(proto: "parameter_values"),
    4: .standard(proto: "failure_policy"),
    5: .standard(proto: "input_artifacts"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_Value>.self, value: &self.parameters) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.gcsOutputDirectory) }()
      case 3: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.Google_Protobuf_Value>.self, value: &self.parameterValues) }()
      case 4: try { try decoder.decodeSingularEnumField(value: &self.failurePolicy) }()
      case 5: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact>.self, value: &self.inputArtifacts) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.parameters.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_Value>.self, value: self.parameters, fieldNumber: 1)
    }
    if !self.gcsOutputDirectory.isEmpty {
      try visitor.visitSingularStringField(value: self.gcsOutputDirectory, fieldNumber: 2)
    }
    if !self.parameterValues.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.Google_Protobuf_Value>.self, value: self.parameterValues, fieldNumber: 3)
    }
    if self.failurePolicy != .unspecified {
      try visitor.visitSingularEnumField(value: self.failurePolicy, fieldNumber: 4)
    }
    if !self.inputArtifacts.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact>.self, value: self.inputArtifacts, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig, rhs: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig) -> Bool {
    if lhs.parameters != rhs.parameters {return false}
    if lhs.gcsOutputDirectory != rhs.gcsOutputDirectory {return false}
    if lhs.parameterValues != rhs.parameterValues {return false}
    if lhs.failurePolicy != rhs.failurePolicy {return false}
    if lhs.inputArtifacts != rhs.inputArtifacts {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.protoMessageName + ".InputArtifact"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "artifact_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {
          if self.kind != nil {try decoder.handleConflictingOneOf()}
          self.kind = .artifactID(v)
        }
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if case .artifactID(let v)? = self.kind {
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact, rhs: Google_Cloud_Aiplatform_V1_PipelineJob.RuntimeConfig.InputArtifact) -> Bool {
    if lhs.kind != rhs.kind {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PipelineTemplateMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .same(proto: "version"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 3: try { try decoder.decodeSingularStringField(value: &self.version) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.version.isEmpty {
      try visitor.visitSingularStringField(value: self.version, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata, rhs: Google_Cloud_Aiplatform_V1_PipelineTemplateMetadata) -> Bool {
    if lhs.version != rhs.version {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineJobDetail: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PipelineJobDetail"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "pipeline_context"),
    2: .standard(proto: "pipeline_run_context"),
    3: .standard(proto: "task_details"),
  ]

  fileprivate class _StorageClass {
    var _pipelineContext: Google_Cloud_Aiplatform_V1_Context? = nil
    var _pipelineRunContext: Google_Cloud_Aiplatform_V1_Context? = nil
    var _taskDetails: [Google_Cloud_Aiplatform_V1_PipelineTaskDetail] = []

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _pipelineContext = source._pipelineContext
      _pipelineRunContext = source._pipelineRunContext
      _taskDetails = source._taskDetails
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._pipelineContext) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._pipelineRunContext) }()
        case 3: try { try decoder.decodeRepeatedMessageField(value: &_storage._taskDetails) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      try { if let v = _storage._pipelineContext {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      } }()
      try { if let v = _storage._pipelineRunContext {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      if !_storage._taskDetails.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._taskDetails, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineJobDetail, rhs: Google_Cloud_Aiplatform_V1_PipelineJobDetail) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._pipelineContext != rhs_storage._pipelineContext {return false}
        if _storage._pipelineRunContext != rhs_storage._pipelineRunContext {return false}
        if _storage._taskDetails != rhs_storage._taskDetails {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskDetail: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PipelineTaskDetail"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "task_id"),
    12: .standard(proto: "parent_task_id"),
    2: .standard(proto: "task_name"),
    3: .standard(proto: "create_time"),
    4: .standard(proto: "start_time"),
    5: .standard(proto: "end_time"),
    6: .standard(proto: "executor_detail"),
    7: .same(proto: "state"),
    8: .same(proto: "execution"),
    9: .same(proto: "error"),
    13: .standard(proto: "pipeline_task_status"),
    10: .same(proto: "inputs"),
    11: .same(proto: "outputs"),
  ]

  fileprivate class _StorageClass {
    var _taskID: Int64 = 0
    var _parentTaskID: Int64 = 0
    var _taskName: String = String()
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _executorDetail: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail? = nil
    var _state: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.State = .unspecified
    var _execution: Google_Cloud_Aiplatform_V1_Execution? = nil
    var _error: Google_Rpc_Status? = nil
    var _pipelineTaskStatus: [Google_Cloud_Aiplatform_V1_PipelineTaskDetail.PipelineTaskStatus] = []
    var _inputs: Dictionary<String,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList> = [:]
    var _outputs: Dictionary<String,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList> = [:]

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _taskID = source._taskID
      _parentTaskID = source._parentTaskID
      _taskName = source._taskName
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _executorDetail = source._executorDetail
      _state = source._state
      _execution = source._execution
      _error = source._error
      _pipelineTaskStatus = source._pipelineTaskStatus
      _inputs = source._inputs
      _outputs = source._outputs
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularInt64Field(value: &_storage._taskID) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._taskName) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._executorDetail) }()
        case 7: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._execution) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 10: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList>.self, value: &_storage._inputs) }()
        case 11: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList>.self, value: &_storage._outputs) }()
        case 12: try { try decoder.decodeSingularInt64Field(value: &_storage._parentTaskID) }()
        case 13: try { try decoder.decodeRepeatedMessageField(value: &_storage._pipelineTaskStatus) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if _storage._taskID != 0 {
        try visitor.visitSingularInt64Field(value: _storage._taskID, fieldNumber: 1)
      }
      if !_storage._taskName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._taskName, fieldNumber: 2)
      }
      try { if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      try { if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      try { if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._executorDetail {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 7)
      }
      try { if let v = _storage._execution {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      } }()
      try { if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
      if !_storage._inputs.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList>.self, value: _storage._inputs, fieldNumber: 10)
      }
      if !_storage._outputs.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList>.self, value: _storage._outputs, fieldNumber: 11)
      }
      if _storage._parentTaskID != 0 {
        try visitor.visitSingularInt64Field(value: _storage._parentTaskID, fieldNumber: 12)
      }
      if !_storage._pipelineTaskStatus.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._pipelineTaskStatus, fieldNumber: 13)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._taskID != rhs_storage._taskID {return false}
        if _storage._parentTaskID != rhs_storage._parentTaskID {return false}
        if _storage._taskName != rhs_storage._taskName {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._executorDetail != rhs_storage._executorDetail {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._execution != rhs_storage._execution {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._pipelineTaskStatus != rhs_storage._pipelineTaskStatus {return false}
        if _storage._inputs != rhs_storage._inputs {return false}
        if _storage._outputs != rhs_storage._outputs {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskDetail.State: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "STATE_UNSPECIFIED"),
    1: .same(proto: "PENDING"),
    2: .same(proto: "RUNNING"),
    3: .same(proto: "SUCCEEDED"),
    4: .same(proto: "CANCEL_PENDING"),
    5: .same(proto: "CANCELLING"),
    6: .same(proto: "CANCELLED"),
    7: .same(proto: "FAILED"),
    8: .same(proto: "SKIPPED"),
    9: .same(proto: "NOT_TRIGGERED"),
  ]
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskDetail.PipelineTaskStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineTaskDetail.protoMessageName + ".PipelineTaskStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "update_time"),
    2: .same(proto: "state"),
    3: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if self.state != .unspecified {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 2)
    }
    try { if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.PipelineTaskStatus, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.PipelineTaskStatus) -> Bool {
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.state != rhs.state {return false}
    if lhs._error != rhs._error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineTaskDetail.protoMessageName + ".ArtifactList"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "artifacts"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.artifacts) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.artifacts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.artifacts, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskDetail.ArtifactList) -> Bool {
    if lhs.artifacts != rhs.artifacts {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PipelineTaskExecutorDetail"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "container_detail"),
    2: .standard(proto: "custom_job_detail"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail?
        var hadOneofValue = false
        if let current = self.details {
          hadOneofValue = true
          if case .containerDetail(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.details = .containerDetail(v)
        }
      }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail?
        var hadOneofValue = false
        if let current = self.details {
          hadOneofValue = true
          if case .customJobDetail(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.details = .customJobDetail(v)
        }
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    switch self.details {
    case .containerDetail?: try {
      guard case .containerDetail(let v)? = self.details else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .customJobDetail?: try {
      guard case .customJobDetail(let v)? = self.details else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail) -> Bool {
    if lhs.details != rhs.details {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.protoMessageName + ".ContainerDetail"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "main_job"),
    2: .standard(proto: "pre_caching_check_job"),
    3: .standard(proto: "failed_main_jobs"),
    4: .standard(proto: "failed_pre_caching_check_jobs"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.mainJob) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.preCachingCheckJob) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.failedMainJobs) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.failedPreCachingCheckJobs) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.mainJob.isEmpty {
      try visitor.visitSingularStringField(value: self.mainJob, fieldNumber: 1)
    }
    if !self.preCachingCheckJob.isEmpty {
      try visitor.visitSingularStringField(value: self.preCachingCheckJob, fieldNumber: 2)
    }
    if !self.failedMainJobs.isEmpty {
      try visitor.visitRepeatedStringField(value: self.failedMainJobs, fieldNumber: 3)
    }
    if !self.failedPreCachingCheckJobs.isEmpty {
      try visitor.visitRepeatedStringField(value: self.failedPreCachingCheckJobs, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.ContainerDetail) -> Bool {
    if lhs.mainJob != rhs.mainJob {return false}
    if lhs.preCachingCheckJob != rhs.preCachingCheckJob {return false}
    if lhs.failedMainJobs != rhs.failedMainJobs {return false}
    if lhs.failedPreCachingCheckJobs != rhs.failedPreCachingCheckJobs {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.protoMessageName + ".CustomJobDetail"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "job"),
    3: .standard(proto: "failed_jobs"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.job) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.failedJobs) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.job.isEmpty {
      try visitor.visitSingularStringField(value: self.job, fieldNumber: 1)
    }
    if !self.failedJobs.isEmpty {
      try visitor.visitRepeatedStringField(value: self.failedJobs, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail, rhs: Google_Cloud_Aiplatform_V1_PipelineTaskExecutorDetail.CustomJobDetail) -> Bool {
    if lhs.job != rhs.job {return false}
    if lhs.failedJobs != rhs.failedJobs {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
